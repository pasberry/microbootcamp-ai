{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Loan Risk with Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:24.983950Z",
     "start_time": "2023-11-26T10:11:24.904442300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prepare the data to be used on a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read the `student_loans.csv` file into a Pandas DataFrame. Review the DataFrame, looking for columns that could eventually define your features and target variables.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.646143400Z",
     "start_time": "2023-11-26T10:11:24.911950700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_history</th>\n",
       "      <th>location_parameter</th>\n",
       "      <th>stem_degree_score</th>\n",
       "      <th>gpa_ranking</th>\n",
       "      <th>alumni_success</th>\n",
       "      <th>study_major_code</th>\n",
       "      <th>time_to_completion</th>\n",
       "      <th>finance_workshop_score</th>\n",
       "      <th>cohort_ranking</th>\n",
       "      <th>total_loan_score</th>\n",
       "      <th>financial_aid_score</th>\n",
       "      <th>credit_ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_history  location_parameter  stem_degree_score  gpa_ranking  \\\n",
       "0              7.4                0.70               0.00          1.9   \n",
       "1              7.8                0.88               0.00          2.6   \n",
       "2              7.8                0.76               0.04          2.3   \n",
       "3             11.2                0.28               0.56          1.9   \n",
       "4              7.4                0.70               0.00          1.9   \n",
       "\n",
       "   alumni_success  study_major_code  time_to_completion  \\\n",
       "0           0.076              11.0                34.0   \n",
       "1           0.098              25.0                67.0   \n",
       "2           0.092              15.0                54.0   \n",
       "3           0.075              17.0                60.0   \n",
       "4           0.076              11.0                34.0   \n",
       "\n",
       "   finance_workshop_score  cohort_ranking  total_loan_score  \\\n",
       "0                  0.9978            3.51              0.56   \n",
       "1                  0.9968            3.20              0.68   \n",
       "2                  0.9970            3.26              0.65   \n",
       "3                  0.9980            3.16              0.58   \n",
       "4                  0.9978            3.51              0.56   \n",
       "\n",
       "   financial_aid_score  credit_ranking  \n",
       "0                  9.4               5  \n",
       "1                  9.8               5  \n",
       "2                  9.8               5  \n",
       "3                  9.8               6  \n",
       "4                  9.4               5  "
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the csv into a Pandas DataFrame\n",
    "file_path = \"https://static.bc-edx.com/mbc/ai/m6/datasets/student_loans.csv\"\n",
    "\n",
    "\n",
    "# Review the DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.646143400Z",
     "start_time": "2023-11-26T10:11:25.627112900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "payment_history           float64\n",
       "location_parameter        float64\n",
       "stem_degree_score         float64\n",
       "gpa_ranking               float64\n",
       "alumni_success            float64\n",
       "study_major_code          float64\n",
       "time_to_completion        float64\n",
       "finance_workshop_score    float64\n",
       "cohort_ranking            float64\n",
       "total_loan_score          float64\n",
       "financial_aid_score       float64\n",
       "credit_ranking              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the data types associated with the columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Using the preprocessed data, create the features (`X`) and target (`y`) datasets. The target dataset should be defined by the preprocessed DataFrame column “credit_ranking”. The remaining columns should define the features dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.646143400Z",
     "start_time": "2023-11-26T10:11:25.627112900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       5\n",
       "1       5\n",
       "2       5\n",
       "3       6\n",
       "4       5\n",
       "       ..\n",
       "1594    5\n",
       "1595    6\n",
       "1596    6\n",
       "1597    5\n",
       "1598    6\n",
       "Name: credit_ranking, Length: 1599, dtype: int64"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the target set y using the credit_ranking column\n",
    "y = df['credit_ranking']\n",
    "\n",
    "# Display a sample of y\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.646143400Z",
     "start_time": "2023-11-26T10:11:25.627112900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "credit_ranking\n",
       "5    681\n",
       "6    638\n",
       "7    199\n",
       "4     53\n",
       "8     18\n",
       "3     10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.647144600Z",
     "start_time": "2023-11-26T10:11:25.627112900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_history</th>\n",
       "      <th>location_parameter</th>\n",
       "      <th>stem_degree_score</th>\n",
       "      <th>gpa_ranking</th>\n",
       "      <th>alumni_success</th>\n",
       "      <th>study_major_code</th>\n",
       "      <th>time_to_completion</th>\n",
       "      <th>finance_workshop_score</th>\n",
       "      <th>cohort_ranking</th>\n",
       "      <th>total_loan_score</th>\n",
       "      <th>financial_aid_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      payment_history  location_parameter  stem_degree_score  gpa_ranking  \\\n",
       "0                 7.4               0.700               0.00          1.9   \n",
       "1                 7.8               0.880               0.00          2.6   \n",
       "2                 7.8               0.760               0.04          2.3   \n",
       "3                11.2               0.280               0.56          1.9   \n",
       "4                 7.4               0.700               0.00          1.9   \n",
       "...               ...                 ...                ...          ...   \n",
       "1594              6.2               0.600               0.08          2.0   \n",
       "1595              5.9               0.550               0.10          2.2   \n",
       "1596              6.3               0.510               0.13          2.3   \n",
       "1597              5.9               0.645               0.12          2.0   \n",
       "1598              6.0               0.310               0.47          3.6   \n",
       "\n",
       "      alumni_success  study_major_code  time_to_completion  \\\n",
       "0              0.076              11.0                34.0   \n",
       "1              0.098              25.0                67.0   \n",
       "2              0.092              15.0                54.0   \n",
       "3              0.075              17.0                60.0   \n",
       "4              0.076              11.0                34.0   \n",
       "...              ...               ...                 ...   \n",
       "1594           0.090              32.0                44.0   \n",
       "1595           0.062              39.0                51.0   \n",
       "1596           0.076              29.0                40.0   \n",
       "1597           0.075              32.0                44.0   \n",
       "1598           0.067              18.0                42.0   \n",
       "\n",
       "      finance_workshop_score  cohort_ranking  total_loan_score  \\\n",
       "0                    0.99780            3.51              0.56   \n",
       "1                    0.99680            3.20              0.68   \n",
       "2                    0.99700            3.26              0.65   \n",
       "3                    0.99800            3.16              0.58   \n",
       "4                    0.99780            3.51              0.56   \n",
       "...                      ...             ...               ...   \n",
       "1594                 0.99490            3.45              0.58   \n",
       "1595                 0.99512            3.52              0.76   \n",
       "1596                 0.99574            3.42              0.75   \n",
       "1597                 0.99547            3.57              0.71   \n",
       "1598                 0.99549            3.39              0.66   \n",
       "\n",
       "      financial_aid_score  \n",
       "0                     9.4  \n",
       "1                     9.8  \n",
       "2                     9.8  \n",
       "3                     9.8  \n",
       "4                     9.4  \n",
       "...                   ...  \n",
       "1594                 10.5  \n",
       "1595                 11.2  \n",
       "1596                 11.0  \n",
       "1597                 10.2  \n",
       "1598                 11.0  \n",
       "\n",
       "[1599 rows x 11 columns]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define features set X by selecting all columns but credit_ranking\n",
    "X = df.copy().drop('credit_ranking', axis= 1)\n",
    "\n",
    "# Review the features DataFrame\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split the features and target sets into training and testing datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.647144600Z",
     "start_time": "2023-11-26T10:11:25.627112900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "# Assign the function a random_state equal to 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.647144600Z",
     "start_time": "2023-11-26T10:11:25.627112900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import SMOTE from imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Instantiate the SMOTE instance \n",
    "# Set the sampling_strategy parameter equal to auto\n",
    "smote_sampler = SMOTE(random_state=1, sampling_strategy='auto')\n",
    "\n",
    "# Fit the training data to the smote_sampler model\n",
    "X_resampled, y_resampled = smote_sampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.679159700Z",
     "start_time": "2023-11-26T10:11:25.627112900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_history</th>\n",
       "      <th>location_parameter</th>\n",
       "      <th>stem_degree_score</th>\n",
       "      <th>gpa_ranking</th>\n",
       "      <th>alumni_success</th>\n",
       "      <th>study_major_code</th>\n",
       "      <th>time_to_completion</th>\n",
       "      <th>finance_workshop_score</th>\n",
       "      <th>cohort_ranking</th>\n",
       "      <th>total_loan_score</th>\n",
       "      <th>financial_aid_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.994100</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>11.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>0.998920</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>9.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.900000</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.993620</td>\n",
       "      <td>3.620000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>12.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>0.997400</td>\n",
       "      <td>3.570000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3061</th>\n",
       "      <td>10.362621</td>\n",
       "      <td>0.337024</td>\n",
       "      <td>0.537786</td>\n",
       "      <td>2.651904</td>\n",
       "      <td>0.072595</td>\n",
       "      <td>5.259522</td>\n",
       "      <td>16.259522</td>\n",
       "      <td>0.996992</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.720071</td>\n",
       "      <td>11.181665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3062</th>\n",
       "      <td>8.923103</td>\n",
       "      <td>0.376303</td>\n",
       "      <td>0.490055</td>\n",
       "      <td>2.704621</td>\n",
       "      <td>0.078728</td>\n",
       "      <td>6.635862</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.995112</td>\n",
       "      <td>3.165897</td>\n",
       "      <td>0.856414</td>\n",
       "      <td>12.145103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063</th>\n",
       "      <td>7.546888</td>\n",
       "      <td>0.365133</td>\n",
       "      <td>0.384333</td>\n",
       "      <td>2.792886</td>\n",
       "      <td>0.066902</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>32.964432</td>\n",
       "      <td>0.995999</td>\n",
       "      <td>3.289466</td>\n",
       "      <td>0.809555</td>\n",
       "      <td>12.043331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3064</th>\n",
       "      <td>7.885545</td>\n",
       "      <td>0.381801</td>\n",
       "      <td>0.406517</td>\n",
       "      <td>3.412087</td>\n",
       "      <td>0.076121</td>\n",
       "      <td>17.746416</td>\n",
       "      <td>38.156386</td>\n",
       "      <td>0.996848</td>\n",
       "      <td>3.365900</td>\n",
       "      <td>0.842654</td>\n",
       "      <td>12.785545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3065</th>\n",
       "      <td>10.688924</td>\n",
       "      <td>0.349574</td>\n",
       "      <td>0.530256</td>\n",
       "      <td>2.601704</td>\n",
       "      <td>0.070085</td>\n",
       "      <td>5.008520</td>\n",
       "      <td>16.008520</td>\n",
       "      <td>0.997193</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.652300</td>\n",
       "      <td>11.005964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3066 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      payment_history  location_parameter  stem_degree_score  gpa_ranking  \\\n",
       "0            7.900000            0.180000           0.400000     1.800000   \n",
       "1            7.000000            0.780000           0.080000     2.000000   \n",
       "2            7.500000            0.770000           0.200000     8.100000   \n",
       "3            5.900000            0.395000           0.130000     2.400000   \n",
       "4            6.400000            0.370000           0.250000     1.900000   \n",
       "...               ...                 ...                ...          ...   \n",
       "3061        10.362621            0.337024           0.537786     2.651904   \n",
       "3062         8.923103            0.376303           0.490055     2.704621   \n",
       "3063         7.546888            0.365133           0.384333     2.792886   \n",
       "3064         7.885545            0.381801           0.406517     3.412087   \n",
       "3065        10.688924            0.349574           0.530256     2.601704   \n",
       "\n",
       "      alumni_success  study_major_code  time_to_completion  \\\n",
       "0           0.062000          7.000000           20.000000   \n",
       "1           0.093000         10.000000           19.000000   \n",
       "2           0.098000         30.000000           92.000000   \n",
       "3           0.056000         14.000000           28.000000   \n",
       "4           0.074000         21.000000           49.000000   \n",
       "...              ...               ...                 ...   \n",
       "3061        0.072595          5.259522           16.259522   \n",
       "3062        0.078728          6.635862           17.000000   \n",
       "3063        0.066902         15.000000           32.964432   \n",
       "3064        0.076121         17.746416           38.156386   \n",
       "3065        0.070085          5.008520           16.008520   \n",
       "\n",
       "      finance_workshop_score  cohort_ranking  total_loan_score  \\\n",
       "0                   0.994100        3.280000          0.700000   \n",
       "1                   0.995600        3.400000          0.470000   \n",
       "2                   0.998920        3.200000          0.580000   \n",
       "3                   0.993620        3.620000          0.670000   \n",
       "4                   0.997400        3.570000          0.620000   \n",
       "...                      ...             ...               ...   \n",
       "3061                0.996992        3.150000          0.720071   \n",
       "3062                0.995112        3.165897          0.856414   \n",
       "3063                0.995999        3.289466          0.809555   \n",
       "3064                0.996848        3.365900          0.842654   \n",
       "3065                0.997193        3.150000          0.652300   \n",
       "\n",
       "      financial_aid_score  \n",
       "0               11.100000  \n",
       "1               10.000000  \n",
       "2                9.200000  \n",
       "3               12.400000  \n",
       "4                9.800000  \n",
       "...                   ...  \n",
       "3061            11.181665  \n",
       "3062            12.145103  \n",
       "3063            12.043331  \n",
       "3064            12.785545  \n",
       "3065            11.005964  \n",
       "\n",
       "[3066 rows x 11 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.679159700Z",
     "start_time": "2023-11-26T10:11:25.667650100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       5\n",
       "1       5\n",
       "2       5\n",
       "3       6\n",
       "4       6\n",
       "       ..\n",
       "3061    8\n",
       "3062    8\n",
       "3063    8\n",
       "3064    8\n",
       "3065    8\n",
       "Name: credit_ranking, Length: 3066, dtype: int64"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.679159700Z",
     "start_time": "2023-11-26T10:11:25.667650100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "credit_ranking\n",
       "5    511\n",
       "6    511\n",
       "7    511\n",
       "4    511\n",
       "3    511\n",
       "8    511\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Use scikit-learn's `StandardScaler` to scale the features data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.679159700Z",
     "start_time": "2023-11-26T10:11:25.667650100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_resampled)\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_train_scaled = X_scaler.transform(X_resampled)\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Compile and Evaluate a Model Using a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a deep neural network by assigning the number of input features, the number of layers, and the number of neurons on each layer using Tensorflow’s Keras.\n",
    "\n",
    "> **Hint** You can start with a two-layer deep neural network model that uses the `relu` activation function for both layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.679159700Z",
     "start_time": "2023-11-26T10:11:25.667650100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of inputs (features) to the model\n",
    "number_input_features = 11\n",
    "\n",
    "# Review the number of features\n",
    "number_input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.679159700Z",
     "start_time": "2023-11-26T10:11:25.667650100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the number of neurons in the output layer\n",
    "output_classes =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.679159700Z",
     "start_time": "2023-11-26T10:11:25.667650100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1 = 8\n",
    "\n",
    "# Review the number hidden nodes in the first layer\n",
    "hidden_nodes_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.679159700Z",
     "start_time": "2023-11-26T10:11:25.667650100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2 = 4\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "hidden_nodes_layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.679159700Z",
     "start_time": "2023-11-26T10:11:25.667650100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the Sequential model instance\n",
    "nn = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.764053100Z",
     "start_time": "2023-11-26T10:11:25.673161500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add the first hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.825908500Z",
     "start_time": "2023-11-26T10:11:25.687160600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add the second hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer2, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.835271600Z",
     "start_time": "2023-11-26T10:11:25.747545300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn.add(Dense(units=output_classes, activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.835271600Z",
     "start_time": "2023-11-26T10:11:25.747545300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_31 (Dense)            (None, 8)                 96        \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137\n",
      "Trainable params: 137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display the Sequential model summary\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compile and fit the model using the `mse` loss function, the `adam` optimizer, and the `mse` evaluation metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:11:25.835271600Z",
     "start_time": "2023-11-26T10:11:25.747545300Z"
    }
   },
   "outputs": [],
   "source": [
    "#opt = SGD(lr=0.001, momentum=0.9)\n",
    "opt  = Adam(learning_rate=0.00001)\n",
    "# Compile the Sequential model\n",
    "nn.compile(loss=\"mean_squared_error\", optimizer=opt, metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:12:13.072006200Z",
     "start_time": "2023-11-26T10:11:25.747545300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 41.9248 - mse: 41.9248 - val_loss: 43.2366 - val_mse: 43.2366\n",
      "Epoch 2/500\n",
      "96/96 [==============================] - 0s 990us/step - loss: 41.7450 - mse: 41.7450 - val_loss: 43.0263 - val_mse: 43.0263\n",
      "Epoch 3/500\n",
      "96/96 [==============================] - 0s 960us/step - loss: 41.5669 - mse: 41.5669 - val_loss: 42.8203 - val_mse: 42.8203\n",
      "Epoch 4/500\n",
      "96/96 [==============================] - 0s 955us/step - loss: 41.3906 - mse: 41.3906 - val_loss: 42.6154 - val_mse: 42.6154\n",
      "Epoch 5/500\n",
      "96/96 [==============================] - 0s 956us/step - loss: 41.2160 - mse: 41.2160 - val_loss: 42.4125 - val_mse: 42.4125\n",
      "Epoch 6/500\n",
      "96/96 [==============================] - 0s 952us/step - loss: 41.0432 - mse: 41.0432 - val_loss: 42.2121 - val_mse: 42.2121\n",
      "Epoch 7/500\n",
      "96/96 [==============================] - 0s 968us/step - loss: 40.8726 - mse: 40.8726 - val_loss: 42.0139 - val_mse: 42.0139\n",
      "Epoch 8/500\n",
      "96/96 [==============================] - 0s 954us/step - loss: 40.7038 - mse: 40.7038 - val_loss: 41.8190 - val_mse: 41.8190\n",
      "Epoch 9/500\n",
      "96/96 [==============================] - 0s 977us/step - loss: 40.5371 - mse: 40.5371 - val_loss: 41.6252 - val_mse: 41.6252\n",
      "Epoch 10/500\n",
      "96/96 [==============================] - 0s 957us/step - loss: 40.3718 - mse: 40.3718 - val_loss: 41.4329 - val_mse: 41.4329\n",
      "Epoch 11/500\n",
      "96/96 [==============================] - 0s 931us/step - loss: 40.2080 - mse: 40.2080 - val_loss: 41.2423 - val_mse: 41.2423\n",
      "Epoch 12/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 40.0456 - mse: 40.0456 - val_loss: 41.0531 - val_mse: 41.0531\n",
      "Epoch 13/500\n",
      "96/96 [==============================] - 0s 947us/step - loss: 39.8843 - mse: 39.8843 - val_loss: 40.8674 - val_mse: 40.8674\n",
      "Epoch 14/500\n",
      "96/96 [==============================] - 0s 948us/step - loss: 39.7246 - mse: 39.7246 - val_loss: 40.6819 - val_mse: 40.6819\n",
      "Epoch 15/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 39.5659 - mse: 39.5659 - val_loss: 40.4988 - val_mse: 40.4988\n",
      "Epoch 16/500\n",
      "96/96 [==============================] - 0s 923us/step - loss: 39.4084 - mse: 39.4084 - val_loss: 40.3167 - val_mse: 40.3167\n",
      "Epoch 17/500\n",
      "96/96 [==============================] - 0s 925us/step - loss: 39.2525 - mse: 39.2525 - val_loss: 40.1344 - val_mse: 40.1344\n",
      "Epoch 18/500\n",
      "96/96 [==============================] - 0s 919us/step - loss: 39.0973 - mse: 39.0973 - val_loss: 39.9564 - val_mse: 39.9564\n",
      "Epoch 19/500\n",
      "96/96 [==============================] - 0s 909us/step - loss: 38.9436 - mse: 38.9436 - val_loss: 39.7779 - val_mse: 39.7779\n",
      "Epoch 20/500\n",
      "96/96 [==============================] - 0s 920us/step - loss: 38.7911 - mse: 38.7911 - val_loss: 39.6009 - val_mse: 39.6009\n",
      "Epoch 21/500\n",
      "96/96 [==============================] - 0s 927us/step - loss: 38.6396 - mse: 38.6396 - val_loss: 39.4263 - val_mse: 39.4263\n",
      "Epoch 22/500\n",
      "96/96 [==============================] - 0s 924us/step - loss: 38.4895 - mse: 38.4895 - val_loss: 39.2521 - val_mse: 39.2521\n",
      "Epoch 23/500\n",
      "96/96 [==============================] - 0s 931us/step - loss: 38.3405 - mse: 38.3405 - val_loss: 39.0803 - val_mse: 39.0803\n",
      "Epoch 24/500\n",
      "96/96 [==============================] - 0s 929us/step - loss: 38.1927 - mse: 38.1927 - val_loss: 38.9078 - val_mse: 38.9078\n",
      "Epoch 25/500\n",
      "96/96 [==============================] - 0s 940us/step - loss: 38.0459 - mse: 38.0459 - val_loss: 38.7392 - val_mse: 38.7392\n",
      "Epoch 26/500\n",
      "96/96 [==============================] - 0s 931us/step - loss: 37.9001 - mse: 37.9001 - val_loss: 38.5703 - val_mse: 38.5703\n",
      "Epoch 27/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 37.7555 - mse: 37.7555 - val_loss: 38.4024 - val_mse: 38.4024\n",
      "Epoch 28/500\n",
      "96/96 [==============================] - 0s 926us/step - loss: 37.6118 - mse: 37.6118 - val_loss: 38.2367 - val_mse: 38.2367\n",
      "Epoch 29/500\n",
      "96/96 [==============================] - 0s 910us/step - loss: 37.4691 - mse: 37.4691 - val_loss: 38.0718 - val_mse: 38.0718\n",
      "Epoch 30/500\n",
      "96/96 [==============================] - 0s 906us/step - loss: 37.3274 - mse: 37.3274 - val_loss: 37.9081 - val_mse: 37.9081\n",
      "Epoch 31/500\n",
      "96/96 [==============================] - 0s 909us/step - loss: 37.1869 - mse: 37.1869 - val_loss: 37.7462 - val_mse: 37.7462\n",
      "Epoch 32/500\n",
      "96/96 [==============================] - 0s 916us/step - loss: 37.0472 - mse: 37.0472 - val_loss: 37.5860 - val_mse: 37.5860\n",
      "Epoch 33/500\n",
      "96/96 [==============================] - 0s 923us/step - loss: 36.9083 - mse: 36.9083 - val_loss: 37.4272 - val_mse: 37.4272\n",
      "Epoch 34/500\n",
      "96/96 [==============================] - 0s 903us/step - loss: 36.7705 - mse: 36.7705 - val_loss: 37.2698 - val_mse: 37.2698\n",
      "Epoch 35/500\n",
      "96/96 [==============================] - 0s 930us/step - loss: 36.6338 - mse: 36.6338 - val_loss: 37.1131 - val_mse: 37.1131\n",
      "Epoch 36/500\n",
      "96/96 [==============================] - 0s 919us/step - loss: 36.4982 - mse: 36.4982 - val_loss: 36.9575 - val_mse: 36.9575\n",
      "Epoch 37/500\n",
      "96/96 [==============================] - 0s 996us/step - loss: 36.3635 - mse: 36.3635 - val_loss: 36.8037 - val_mse: 36.8037\n",
      "Epoch 38/500\n",
      "96/96 [==============================] - 0s 940us/step - loss: 36.2300 - mse: 36.2300 - val_loss: 36.6510 - val_mse: 36.6510\n",
      "Epoch 39/500\n",
      "96/96 [==============================] - 0s 933us/step - loss: 36.0972 - mse: 36.0972 - val_loss: 36.4989 - val_mse: 36.4989\n",
      "Epoch 40/500\n",
      "96/96 [==============================] - 0s 952us/step - loss: 35.9655 - mse: 35.9655 - val_loss: 36.3481 - val_mse: 36.3481\n",
      "Epoch 41/500\n",
      "96/96 [==============================] - 0s 921us/step - loss: 35.8348 - mse: 35.8348 - val_loss: 36.1989 - val_mse: 36.1989\n",
      "Epoch 42/500\n",
      "96/96 [==============================] - 0s 934us/step - loss: 35.7052 - mse: 35.7052 - val_loss: 36.0512 - val_mse: 36.0512\n",
      "Epoch 43/500\n",
      "96/96 [==============================] - 0s 913us/step - loss: 35.5763 - mse: 35.5763 - val_loss: 35.9040 - val_mse: 35.9040\n",
      "Epoch 44/500\n",
      "96/96 [==============================] - 0s 922us/step - loss: 35.4482 - mse: 35.4482 - val_loss: 35.7577 - val_mse: 35.7577\n",
      "Epoch 45/500\n",
      "96/96 [==============================] - 0s 942us/step - loss: 35.3210 - mse: 35.3210 - val_loss: 35.6122 - val_mse: 35.6122\n",
      "Epoch 46/500\n",
      "96/96 [==============================] - 0s 924us/step - loss: 35.1948 - mse: 35.1948 - val_loss: 35.4675 - val_mse: 35.4675\n",
      "Epoch 47/500\n",
      "96/96 [==============================] - 0s 932us/step - loss: 35.0691 - mse: 35.0691 - val_loss: 35.3251 - val_mse: 35.3251\n",
      "Epoch 48/500\n",
      "96/96 [==============================] - 0s 962us/step - loss: 34.9444 - mse: 34.9444 - val_loss: 35.1823 - val_mse: 35.1823\n",
      "Epoch 49/500\n",
      "96/96 [==============================] - 0s 964us/step - loss: 34.8210 - mse: 34.8210 - val_loss: 35.0407 - val_mse: 35.0407\n",
      "Epoch 50/500\n",
      "96/96 [==============================] - 0s 963us/step - loss: 34.6983 - mse: 34.6983 - val_loss: 34.9001 - val_mse: 34.9001\n",
      "Epoch 51/500\n",
      "96/96 [==============================] - 0s 946us/step - loss: 34.5765 - mse: 34.5765 - val_loss: 34.7597 - val_mse: 34.7597\n",
      "Epoch 52/500\n",
      "96/96 [==============================] - 0s 931us/step - loss: 34.4553 - mse: 34.4553 - val_loss: 34.6197 - val_mse: 34.6197\n",
      "Epoch 53/500\n",
      "96/96 [==============================] - 0s 967us/step - loss: 34.3346 - mse: 34.3346 - val_loss: 34.4813 - val_mse: 34.4813\n",
      "Epoch 54/500\n",
      "96/96 [==============================] - 0s 970us/step - loss: 34.2145 - mse: 34.2145 - val_loss: 34.3434 - val_mse: 34.3434\n",
      "Epoch 55/500\n",
      "96/96 [==============================] - 0s 965us/step - loss: 34.0949 - mse: 34.0949 - val_loss: 34.2072 - val_mse: 34.2072\n",
      "Epoch 56/500\n",
      "96/96 [==============================] - 0s 945us/step - loss: 33.9760 - mse: 33.9760 - val_loss: 34.0705 - val_mse: 34.0705\n",
      "Epoch 57/500\n",
      "96/96 [==============================] - 0s 955us/step - loss: 33.8578 - mse: 33.8578 - val_loss: 33.9353 - val_mse: 33.9353\n",
      "Epoch 58/500\n",
      "96/96 [==============================] - 0s 940us/step - loss: 33.7405 - mse: 33.7405 - val_loss: 33.8010 - val_mse: 33.8010\n",
      "Epoch 59/500\n",
      "96/96 [==============================] - 0s 981us/step - loss: 33.6240 - mse: 33.6240 - val_loss: 33.6665 - val_mse: 33.6665\n",
      "Epoch 60/500\n",
      "96/96 [==============================] - 0s 963us/step - loss: 33.5078 - mse: 33.5078 - val_loss: 33.5343 - val_mse: 33.5343\n",
      "Epoch 61/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 33.3923 - mse: 33.3923 - val_loss: 33.4006 - val_mse: 33.4006\n",
      "Epoch 62/500\n",
      "96/96 [==============================] - 0s 968us/step - loss: 33.2772 - mse: 33.2772 - val_loss: 33.2690 - val_mse: 33.2690\n",
      "Epoch 63/500\n",
      "96/96 [==============================] - 0s 950us/step - loss: 33.1629 - mse: 33.1629 - val_loss: 33.1366 - val_mse: 33.1366\n",
      "Epoch 64/500\n",
      "96/96 [==============================] - 0s 977us/step - loss: 33.0490 - mse: 33.0490 - val_loss: 33.0058 - val_mse: 33.0058\n",
      "Epoch 65/500\n",
      "96/96 [==============================] - 0s 958us/step - loss: 32.9358 - mse: 32.9358 - val_loss: 32.8755 - val_mse: 32.8755\n",
      "Epoch 66/500\n",
      "96/96 [==============================] - 0s 945us/step - loss: 32.8231 - mse: 32.8231 - val_loss: 32.7466 - val_mse: 32.7466\n",
      "Epoch 67/500\n",
      "96/96 [==============================] - 0s 983us/step - loss: 32.7112 - mse: 32.7112 - val_loss: 32.6175 - val_mse: 32.6175\n",
      "Epoch 68/500\n",
      "96/96 [==============================] - 0s 962us/step - loss: 32.5998 - mse: 32.5998 - val_loss: 32.4896 - val_mse: 32.4896\n",
      "Epoch 69/500\n",
      "96/96 [==============================] - 0s 912us/step - loss: 32.4890 - mse: 32.4890 - val_loss: 32.3622 - val_mse: 32.3622\n",
      "Epoch 70/500\n",
      "96/96 [==============================] - 0s 916us/step - loss: 32.3788 - mse: 32.3788 - val_loss: 32.2350 - val_mse: 32.2350\n",
      "Epoch 71/500\n",
      "96/96 [==============================] - 0s 931us/step - loss: 32.2691 - mse: 32.2691 - val_loss: 32.1088 - val_mse: 32.1088\n",
      "Epoch 72/500\n",
      "96/96 [==============================] - 0s 927us/step - loss: 32.1598 - mse: 32.1598 - val_loss: 31.9833 - val_mse: 31.9833\n",
      "Epoch 73/500\n",
      "96/96 [==============================] - 0s 979us/step - loss: 32.0509 - mse: 32.0509 - val_loss: 31.8584 - val_mse: 31.8584\n",
      "Epoch 74/500\n",
      "96/96 [==============================] - 0s 958us/step - loss: 31.9425 - mse: 31.9425 - val_loss: 31.7334 - val_mse: 31.7334\n",
      "Epoch 75/500\n",
      "96/96 [==============================] - 0s 934us/step - loss: 31.8345 - mse: 31.8345 - val_loss: 31.6098 - val_mse: 31.6098\n",
      "Epoch 76/500\n",
      "96/96 [==============================] - 0s 912us/step - loss: 31.7272 - mse: 31.7272 - val_loss: 31.4867 - val_mse: 31.4867\n",
      "Epoch 77/500\n",
      "96/96 [==============================] - 0s 944us/step - loss: 31.6205 - mse: 31.6205 - val_loss: 31.3639 - val_mse: 31.3639\n",
      "Epoch 78/500\n",
      "96/96 [==============================] - 0s 934us/step - loss: 31.5142 - mse: 31.5142 - val_loss: 31.2418 - val_mse: 31.2418\n",
      "Epoch 79/500\n",
      "96/96 [==============================] - 0s 946us/step - loss: 31.4083 - mse: 31.4083 - val_loss: 31.1204 - val_mse: 31.1204\n",
      "Epoch 80/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 31.3029 - mse: 31.3029 - val_loss: 30.9994 - val_mse: 30.9994\n",
      "Epoch 81/500\n",
      "96/96 [==============================] - 0s 944us/step - loss: 31.1980 - mse: 31.1980 - val_loss: 30.8786 - val_mse: 30.8786\n",
      "Epoch 82/500\n",
      "96/96 [==============================] - 0s 930us/step - loss: 31.0935 - mse: 31.0935 - val_loss: 30.7587 - val_mse: 30.7587\n",
      "Epoch 83/500\n",
      "96/96 [==============================] - 0s 940us/step - loss: 30.9894 - mse: 30.9894 - val_loss: 30.6389 - val_mse: 30.6389\n",
      "Epoch 84/500\n",
      "96/96 [==============================] - 0s 932us/step - loss: 30.8858 - mse: 30.8858 - val_loss: 30.5201 - val_mse: 30.5201\n",
      "Epoch 85/500\n",
      "96/96 [==============================] - 0s 979us/step - loss: 30.7826 - mse: 30.7826 - val_loss: 30.4026 - val_mse: 30.4026\n",
      "Epoch 86/500\n",
      "96/96 [==============================] - 0s 962us/step - loss: 30.6800 - mse: 30.6800 - val_loss: 30.2841 - val_mse: 30.2841\n",
      "Epoch 87/500\n",
      "96/96 [==============================] - 0s 968us/step - loss: 30.5779 - mse: 30.5779 - val_loss: 30.1669 - val_mse: 30.1669\n",
      "Epoch 88/500\n",
      "96/96 [==============================] - 0s 942us/step - loss: 30.4764 - mse: 30.4764 - val_loss: 30.0499 - val_mse: 30.0499\n",
      "Epoch 89/500\n",
      "96/96 [==============================] - 0s 940us/step - loss: 30.3752 - mse: 30.3752 - val_loss: 29.9341 - val_mse: 29.9341\n",
      "Epoch 90/500\n",
      "96/96 [==============================] - 0s 963us/step - loss: 30.2746 - mse: 30.2746 - val_loss: 29.8187 - val_mse: 29.8187\n",
      "Epoch 91/500\n",
      "96/96 [==============================] - 0s 995us/step - loss: 30.1744 - mse: 30.1744 - val_loss: 29.7031 - val_mse: 29.7031\n",
      "Epoch 92/500\n",
      "96/96 [==============================] - 0s 935us/step - loss: 30.0746 - mse: 30.0746 - val_loss: 29.5894 - val_mse: 29.5894\n",
      "Epoch 93/500\n",
      "96/96 [==============================] - 0s 936us/step - loss: 29.9753 - mse: 29.9753 - val_loss: 29.4756 - val_mse: 29.4756\n",
      "Epoch 94/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 29.8764 - mse: 29.8764 - val_loss: 29.3624 - val_mse: 29.3624\n",
      "Epoch 95/500\n",
      "96/96 [==============================] - 0s 935us/step - loss: 29.7780 - mse: 29.7780 - val_loss: 29.2499 - val_mse: 29.2499\n",
      "Epoch 96/500\n",
      "96/96 [==============================] - 0s 946us/step - loss: 29.6798 - mse: 29.6798 - val_loss: 29.1384 - val_mse: 29.1384\n",
      "Epoch 97/500\n",
      "96/96 [==============================] - 0s 932us/step - loss: 29.5822 - mse: 29.5822 - val_loss: 29.0263 - val_mse: 29.0263\n",
      "Epoch 98/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 29.4847 - mse: 29.4847 - val_loss: 28.9152 - val_mse: 28.9152\n",
      "Epoch 99/500\n",
      "96/96 [==============================] - 0s 933us/step - loss: 29.3877 - mse: 29.3877 - val_loss: 28.8053 - val_mse: 28.8053\n",
      "Epoch 100/500\n",
      "96/96 [==============================] - 0s 923us/step - loss: 29.2911 - mse: 29.2911 - val_loss: 28.6951 - val_mse: 28.6951\n",
      "Epoch 101/500\n",
      "96/96 [==============================] - 0s 934us/step - loss: 29.1948 - mse: 29.1948 - val_loss: 28.5855 - val_mse: 28.5855\n",
      "Epoch 102/500\n",
      "96/96 [==============================] - 0s 927us/step - loss: 29.0988 - mse: 29.0988 - val_loss: 28.4768 - val_mse: 28.4768\n",
      "Epoch 103/500\n",
      "96/96 [==============================] - 0s 929us/step - loss: 29.0032 - mse: 29.0032 - val_loss: 28.3681 - val_mse: 28.3681\n",
      "Epoch 104/500\n",
      "96/96 [==============================] - 0s 916us/step - loss: 28.9078 - mse: 28.9078 - val_loss: 28.2604 - val_mse: 28.2604\n",
      "Epoch 105/500\n",
      "96/96 [==============================] - 0s 936us/step - loss: 28.8126 - mse: 28.8126 - val_loss: 28.1535 - val_mse: 28.1535\n",
      "Epoch 106/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 28.7176 - mse: 28.7176 - val_loss: 28.0457 - val_mse: 28.0457\n",
      "Epoch 107/500\n",
      "96/96 [==============================] - 0s 918us/step - loss: 28.6229 - mse: 28.6229 - val_loss: 27.9388 - val_mse: 27.9388\n",
      "Epoch 108/500\n",
      "96/96 [==============================] - 0s 958us/step - loss: 28.5284 - mse: 28.5284 - val_loss: 27.8316 - val_mse: 27.8316\n",
      "Epoch 109/500\n",
      "96/96 [==============================] - 0s 983us/step - loss: 28.4341 - mse: 28.4341 - val_loss: 27.7258 - val_mse: 27.7258\n",
      "Epoch 110/500\n",
      "96/96 [==============================] - 0s 932us/step - loss: 28.3401 - mse: 28.3401 - val_loss: 27.6202 - val_mse: 27.6202\n",
      "Epoch 111/500\n",
      "96/96 [==============================] - 0s 930us/step - loss: 28.2465 - mse: 28.2465 - val_loss: 27.5147 - val_mse: 27.5147\n",
      "Epoch 112/500\n",
      "96/96 [==============================] - 0s 916us/step - loss: 28.1531 - mse: 28.1531 - val_loss: 27.4096 - val_mse: 27.4096\n",
      "Epoch 113/500\n",
      "96/96 [==============================] - 0s 941us/step - loss: 28.0599 - mse: 28.0599 - val_loss: 27.3053 - val_mse: 27.3053\n",
      "Epoch 114/500\n",
      "96/96 [==============================] - 0s 977us/step - loss: 27.9670 - mse: 27.9670 - val_loss: 27.2015 - val_mse: 27.2015\n",
      "Epoch 115/500\n",
      "96/96 [==============================] - 0s 934us/step - loss: 27.8746 - mse: 27.8746 - val_loss: 27.0976 - val_mse: 27.0976\n",
      "Epoch 116/500\n",
      "96/96 [==============================] - 0s 922us/step - loss: 27.7825 - mse: 27.7825 - val_loss: 26.9937 - val_mse: 26.9937\n",
      "Epoch 117/500\n",
      "96/96 [==============================] - 0s 927us/step - loss: 27.6909 - mse: 27.6909 - val_loss: 26.8907 - val_mse: 26.8907\n",
      "Epoch 118/500\n",
      "96/96 [==============================] - 0s 911us/step - loss: 27.5997 - mse: 27.5997 - val_loss: 26.7882 - val_mse: 26.7882\n",
      "Epoch 119/500\n",
      "96/96 [==============================] - 0s 943us/step - loss: 27.5088 - mse: 27.5088 - val_loss: 26.6860 - val_mse: 26.6860\n",
      "Epoch 120/500\n",
      "96/96 [==============================] - 0s 939us/step - loss: 27.4181 - mse: 27.4181 - val_loss: 26.5840 - val_mse: 26.5840\n",
      "Epoch 121/500\n",
      "96/96 [==============================] - 0s 964us/step - loss: 27.3274 - mse: 27.3274 - val_loss: 26.4822 - val_mse: 26.4822\n",
      "Epoch 122/500\n",
      "96/96 [==============================] - 0s 931us/step - loss: 27.2368 - mse: 27.2368 - val_loss: 26.3806 - val_mse: 26.3806\n",
      "Epoch 123/500\n",
      "96/96 [==============================] - 0s 996us/step - loss: 27.1462 - mse: 27.1462 - val_loss: 26.2796 - val_mse: 26.2796\n",
      "Epoch 124/500\n",
      "96/96 [==============================] - 0s 947us/step - loss: 27.0559 - mse: 27.0559 - val_loss: 26.1786 - val_mse: 26.1786\n",
      "Epoch 125/500\n",
      "96/96 [==============================] - 0s 941us/step - loss: 26.9657 - mse: 26.9657 - val_loss: 26.0779 - val_mse: 26.0779\n",
      "Epoch 126/500\n",
      "96/96 [==============================] - 0s 951us/step - loss: 26.8756 - mse: 26.8756 - val_loss: 25.9773 - val_mse: 25.9773\n",
      "Epoch 127/500\n",
      "96/96 [==============================] - 0s 937us/step - loss: 26.7857 - mse: 26.7857 - val_loss: 25.8770 - val_mse: 25.8770\n",
      "Epoch 128/500\n",
      "96/96 [==============================] - 0s 954us/step - loss: 26.6958 - mse: 26.6958 - val_loss: 25.7778 - val_mse: 25.7778\n",
      "Epoch 129/500\n",
      "96/96 [==============================] - 0s 964us/step - loss: 26.6060 - mse: 26.6060 - val_loss: 25.6778 - val_mse: 25.6778\n",
      "Epoch 130/500\n",
      "96/96 [==============================] - 0s 958us/step - loss: 26.5163 - mse: 26.5163 - val_loss: 25.5782 - val_mse: 25.5782\n",
      "Epoch 131/500\n",
      "96/96 [==============================] - 0s 959us/step - loss: 26.4268 - mse: 26.4268 - val_loss: 25.4786 - val_mse: 25.4786\n",
      "Epoch 132/500\n",
      "96/96 [==============================] - 0s 975us/step - loss: 26.3371 - mse: 26.3371 - val_loss: 25.3805 - val_mse: 25.3805\n",
      "Epoch 133/500\n",
      "96/96 [==============================] - 0s 972us/step - loss: 26.2477 - mse: 26.2477 - val_loss: 25.2812 - val_mse: 25.2812\n",
      "Epoch 134/500\n",
      "96/96 [==============================] - 0s 972us/step - loss: 26.1582 - mse: 26.1582 - val_loss: 25.1837 - val_mse: 25.1837\n",
      "Epoch 135/500\n",
      "96/96 [==============================] - 0s 940us/step - loss: 26.0689 - mse: 26.0689 - val_loss: 25.0852 - val_mse: 25.0852\n",
      "Epoch 136/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 25.9795 - mse: 25.9795 - val_loss: 24.9867 - val_mse: 24.9867\n",
      "Epoch 137/500\n",
      "96/96 [==============================] - 0s 944us/step - loss: 25.8902 - mse: 25.8902 - val_loss: 24.8898 - val_mse: 24.8898\n",
      "Epoch 138/500\n",
      "96/96 [==============================] - 0s 947us/step - loss: 25.8010 - mse: 25.8010 - val_loss: 24.7919 - val_mse: 24.7919\n",
      "Epoch 139/500\n",
      "96/96 [==============================] - 0s 933us/step - loss: 25.7118 - mse: 25.7118 - val_loss: 24.6949 - val_mse: 24.6949\n",
      "Epoch 140/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 25.6228 - mse: 25.6228 - val_loss: 24.5978 - val_mse: 24.5978\n",
      "Epoch 141/500\n",
      "96/96 [==============================] - 0s 934us/step - loss: 25.5338 - mse: 25.5338 - val_loss: 24.5009 - val_mse: 24.5009\n",
      "Epoch 142/500\n",
      "96/96 [==============================] - 0s 926us/step - loss: 25.4447 - mse: 25.4447 - val_loss: 24.4042 - val_mse: 24.4042\n",
      "Epoch 143/500\n",
      "96/96 [==============================] - 0s 923us/step - loss: 25.3556 - mse: 25.3556 - val_loss: 24.3073 - val_mse: 24.3073\n",
      "Epoch 144/500\n",
      "96/96 [==============================] - 0s 922us/step - loss: 25.2661 - mse: 25.2661 - val_loss: 24.2114 - val_mse: 24.2114\n",
      "Epoch 145/500\n",
      "96/96 [==============================] - 0s 967us/step - loss: 25.1766 - mse: 25.1766 - val_loss: 24.1149 - val_mse: 24.1149\n",
      "Epoch 146/500\n",
      "96/96 [==============================] - 0s 944us/step - loss: 25.0870 - mse: 25.0870 - val_loss: 24.0186 - val_mse: 24.0186\n",
      "Epoch 147/500\n",
      "96/96 [==============================] - 0s 962us/step - loss: 24.9974 - mse: 24.9974 - val_loss: 23.9228 - val_mse: 23.9228\n",
      "Epoch 148/500\n",
      "96/96 [==============================] - 0s 937us/step - loss: 24.9077 - mse: 24.9077 - val_loss: 23.8271 - val_mse: 23.8271\n",
      "Epoch 149/500\n",
      "96/96 [==============================] - 0s 956us/step - loss: 24.8180 - mse: 24.8180 - val_loss: 23.7317 - val_mse: 23.7317\n",
      "Epoch 150/500\n",
      "96/96 [==============================] - 0s 936us/step - loss: 24.7282 - mse: 24.7282 - val_loss: 23.6357 - val_mse: 23.6357\n",
      "Epoch 151/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 24.6382 - mse: 24.6382 - val_loss: 23.5407 - val_mse: 23.5407\n",
      "Epoch 152/500\n",
      "96/96 [==============================] - 0s 948us/step - loss: 24.5483 - mse: 24.5483 - val_loss: 23.4451 - val_mse: 23.4451\n",
      "Epoch 153/500\n",
      "96/96 [==============================] - 0s 930us/step - loss: 24.4582 - mse: 24.4582 - val_loss: 23.3502 - val_mse: 23.3502\n",
      "Epoch 154/500\n",
      "96/96 [==============================] - 0s 953us/step - loss: 24.3681 - mse: 24.3681 - val_loss: 23.2556 - val_mse: 23.2556\n",
      "Epoch 155/500\n",
      "96/96 [==============================] - 0s 947us/step - loss: 24.2777 - mse: 24.2777 - val_loss: 23.1612 - val_mse: 23.1612\n",
      "Epoch 156/500\n",
      "96/96 [==============================] - 0s 921us/step - loss: 24.1870 - mse: 24.1870 - val_loss: 23.0663 - val_mse: 23.0663\n",
      "Epoch 157/500\n",
      "96/96 [==============================] - 0s 942us/step - loss: 24.0963 - mse: 24.0963 - val_loss: 22.9718 - val_mse: 22.9718\n",
      "Epoch 158/500\n",
      "96/96 [==============================] - 0s 943us/step - loss: 24.0054 - mse: 24.0054 - val_loss: 22.8775 - val_mse: 22.8775\n",
      "Epoch 159/500\n",
      "96/96 [==============================] - 0s 995us/step - loss: 23.9143 - mse: 23.9143 - val_loss: 22.7830 - val_mse: 22.7830\n",
      "Epoch 160/500\n",
      "96/96 [==============================] - 0s 930us/step - loss: 23.8232 - mse: 23.8232 - val_loss: 22.6893 - val_mse: 22.6893\n",
      "Epoch 161/500\n",
      "96/96 [==============================] - 0s 927us/step - loss: 23.7320 - mse: 23.7320 - val_loss: 22.5951 - val_mse: 22.5951\n",
      "Epoch 162/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 23.6406 - mse: 23.6406 - val_loss: 22.5009 - val_mse: 22.5009\n",
      "Epoch 163/500\n",
      "96/96 [==============================] - 0s 923us/step - loss: 23.5492 - mse: 23.5492 - val_loss: 22.4064 - val_mse: 22.4064\n",
      "Epoch 164/500\n",
      "96/96 [==============================] - 0s 915us/step - loss: 23.4577 - mse: 23.4577 - val_loss: 22.3122 - val_mse: 22.3122\n",
      "Epoch 165/500\n",
      "96/96 [==============================] - 0s 913us/step - loss: 23.3661 - mse: 23.3661 - val_loss: 22.2178 - val_mse: 22.2178\n",
      "Epoch 166/500\n",
      "96/96 [==============================] - 0s 965us/step - loss: 23.2743 - mse: 23.2743 - val_loss: 22.1237 - val_mse: 22.1237\n",
      "Epoch 167/500\n",
      "96/96 [==============================] - 0s 973us/step - loss: 23.1824 - mse: 23.1824 - val_loss: 22.0297 - val_mse: 22.0297\n",
      "Epoch 168/500\n",
      "96/96 [==============================] - 0s 967us/step - loss: 23.0904 - mse: 23.0904 - val_loss: 21.9365 - val_mse: 21.9365\n",
      "Epoch 169/500\n",
      "96/96 [==============================] - 0s 940us/step - loss: 22.9983 - mse: 22.9983 - val_loss: 21.8424 - val_mse: 21.8424\n",
      "Epoch 170/500\n",
      "96/96 [==============================] - 0s 942us/step - loss: 22.9059 - mse: 22.9059 - val_loss: 21.7491 - val_mse: 21.7491\n",
      "Epoch 171/500\n",
      "96/96 [==============================] - 0s 942us/step - loss: 22.8134 - mse: 22.8134 - val_loss: 21.6553 - val_mse: 21.6553\n",
      "Epoch 172/500\n",
      "96/96 [==============================] - 0s 932us/step - loss: 22.7207 - mse: 22.7207 - val_loss: 21.5616 - val_mse: 21.5616\n",
      "Epoch 173/500\n",
      "96/96 [==============================] - 0s 977us/step - loss: 22.6277 - mse: 22.6277 - val_loss: 21.4687 - val_mse: 21.4687\n",
      "Epoch 174/500\n",
      "96/96 [==============================] - 0s 975us/step - loss: 22.5348 - mse: 22.5348 - val_loss: 21.3751 - val_mse: 21.3751\n",
      "Epoch 175/500\n",
      "96/96 [==============================] - 0s 940us/step - loss: 22.4416 - mse: 22.4416 - val_loss: 21.2821 - val_mse: 21.2821\n",
      "Epoch 176/500\n",
      "96/96 [==============================] - 0s 924us/step - loss: 22.3484 - mse: 22.3484 - val_loss: 21.1885 - val_mse: 21.1885\n",
      "Epoch 177/500\n",
      "96/96 [==============================] - 0s 923us/step - loss: 22.2552 - mse: 22.2552 - val_loss: 21.0956 - val_mse: 21.0956\n",
      "Epoch 178/500\n",
      "96/96 [==============================] - 0s 916us/step - loss: 22.1618 - mse: 22.1618 - val_loss: 21.0028 - val_mse: 21.0028\n",
      "Epoch 179/500\n",
      "96/96 [==============================] - 0s 934us/step - loss: 22.0684 - mse: 22.0684 - val_loss: 20.9098 - val_mse: 20.9098\n",
      "Epoch 180/500\n",
      "96/96 [==============================] - 0s 924us/step - loss: 21.9747 - mse: 21.9747 - val_loss: 20.8167 - val_mse: 20.8167\n",
      "Epoch 181/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 21.8810 - mse: 21.8810 - val_loss: 20.7243 - val_mse: 20.7243\n",
      "Epoch 182/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 21.7872 - mse: 21.7872 - val_loss: 20.6322 - val_mse: 20.6322\n",
      "Epoch 183/500\n",
      "96/96 [==============================] - 0s 974us/step - loss: 21.6931 - mse: 21.6931 - val_loss: 20.5398 - val_mse: 20.5398\n",
      "Epoch 184/500\n",
      "96/96 [==============================] - 0s 991us/step - loss: 21.5989 - mse: 21.5989 - val_loss: 20.4476 - val_mse: 20.4476\n",
      "Epoch 185/500\n",
      "96/96 [==============================] - 0s 985us/step - loss: 21.5046 - mse: 21.5046 - val_loss: 20.3557 - val_mse: 20.3557\n",
      "Epoch 186/500\n",
      "96/96 [==============================] - 0s 968us/step - loss: 21.4102 - mse: 21.4102 - val_loss: 20.2636 - val_mse: 20.2636\n",
      "Epoch 187/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 21.3158 - mse: 21.3158 - val_loss: 20.1718 - val_mse: 20.1718\n",
      "Epoch 188/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 21.2211 - mse: 21.2211 - val_loss: 20.0800 - val_mse: 20.0800\n",
      "Epoch 189/500\n",
      "96/96 [==============================] - 0s 989us/step - loss: 21.1265 - mse: 21.1265 - val_loss: 19.9884 - val_mse: 19.9884\n",
      "Epoch 190/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 21.0316 - mse: 21.0316 - val_loss: 19.8968 - val_mse: 19.8968\n",
      "Epoch 191/500\n",
      "96/96 [==============================] - 0s 996us/step - loss: 20.9368 - mse: 20.9368 - val_loss: 19.8050 - val_mse: 19.8050\n",
      "Epoch 192/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 20.8419 - mse: 20.8419 - val_loss: 19.7134 - val_mse: 19.7134\n",
      "Epoch 193/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 20.7469 - mse: 20.7469 - val_loss: 19.6220 - val_mse: 19.6220\n",
      "Epoch 194/500\n",
      "96/96 [==============================] - 0s 991us/step - loss: 20.6519 - mse: 20.6519 - val_loss: 19.5305 - val_mse: 19.5305\n",
      "Epoch 195/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 20.5567 - mse: 20.5567 - val_loss: 19.4396 - val_mse: 19.4396\n",
      "Epoch 196/500\n",
      "96/96 [==============================] - 0s 940us/step - loss: 20.4616 - mse: 20.4616 - val_loss: 19.3484 - val_mse: 19.3484\n",
      "Epoch 197/500\n",
      "96/96 [==============================] - 0s 930us/step - loss: 20.3665 - mse: 20.3665 - val_loss: 19.2568 - val_mse: 19.2568\n",
      "Epoch 198/500\n",
      "96/96 [==============================] - 0s 942us/step - loss: 20.2712 - mse: 20.2712 - val_loss: 19.1660 - val_mse: 19.1660\n",
      "Epoch 199/500\n",
      "96/96 [==============================] - 0s 925us/step - loss: 20.1760 - mse: 20.1760 - val_loss: 19.0753 - val_mse: 19.0753\n",
      "Epoch 200/500\n",
      "96/96 [==============================] - 0s 933us/step - loss: 20.0806 - mse: 20.0806 - val_loss: 18.9846 - val_mse: 18.9846\n",
      "Epoch 201/500\n",
      "96/96 [==============================] - 0s 919us/step - loss: 19.9851 - mse: 19.9851 - val_loss: 18.8937 - val_mse: 18.8937\n",
      "Epoch 202/500\n",
      "96/96 [==============================] - 0s 949us/step - loss: 19.8897 - mse: 19.8897 - val_loss: 18.8034 - val_mse: 18.8034\n",
      "Epoch 203/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 19.7942 - mse: 19.7942 - val_loss: 18.7134 - val_mse: 18.7134\n",
      "Epoch 204/500\n",
      "96/96 [==============================] - 0s 979us/step - loss: 19.6986 - mse: 19.6986 - val_loss: 18.6229 - val_mse: 18.6229\n",
      "Epoch 205/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 19.6030 - mse: 19.6030 - val_loss: 18.5331 - val_mse: 18.5331\n",
      "Epoch 206/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 19.5074 - mse: 19.5074 - val_loss: 18.4429 - val_mse: 18.4429\n",
      "Epoch 207/500\n",
      "96/96 [==============================] - 0s 979us/step - loss: 19.4115 - mse: 19.4115 - val_loss: 18.3534 - val_mse: 18.3534\n",
      "Epoch 208/500\n",
      "96/96 [==============================] - 0s 947us/step - loss: 19.3158 - mse: 19.3158 - val_loss: 18.2635 - val_mse: 18.2635\n",
      "Epoch 209/500\n",
      "96/96 [==============================] - 0s 963us/step - loss: 19.2200 - mse: 19.2200 - val_loss: 18.1746 - val_mse: 18.1746\n",
      "Epoch 210/500\n",
      "96/96 [==============================] - 0s 972us/step - loss: 19.1241 - mse: 19.1241 - val_loss: 18.0856 - val_mse: 18.0856\n",
      "Epoch 211/500\n",
      "96/96 [==============================] - 0s 987us/step - loss: 19.0283 - mse: 19.0283 - val_loss: 17.9960 - val_mse: 17.9960\n",
      "Epoch 212/500\n",
      "96/96 [==============================] - 0s 987us/step - loss: 18.9324 - mse: 18.9324 - val_loss: 17.9068 - val_mse: 17.9068\n",
      "Epoch 213/500\n",
      "96/96 [==============================] - 0s 949us/step - loss: 18.8363 - mse: 18.8363 - val_loss: 17.8175 - val_mse: 17.8175\n",
      "Epoch 214/500\n",
      "96/96 [==============================] - 0s 956us/step - loss: 18.7403 - mse: 18.7403 - val_loss: 17.7283 - val_mse: 17.7283\n",
      "Epoch 215/500\n",
      "96/96 [==============================] - 0s 953us/step - loss: 18.6441 - mse: 18.6441 - val_loss: 17.6395 - val_mse: 17.6395\n",
      "Epoch 216/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 18.5480 - mse: 18.5480 - val_loss: 17.5506 - val_mse: 17.5506\n",
      "Epoch 217/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 18.4519 - mse: 18.4519 - val_loss: 17.4620 - val_mse: 17.4620\n",
      "Epoch 218/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 18.3557 - mse: 18.3557 - val_loss: 17.3734 - val_mse: 17.3734\n",
      "Epoch 219/500\n",
      "96/96 [==============================] - 0s 985us/step - loss: 18.2596 - mse: 18.2596 - val_loss: 17.2850 - val_mse: 17.2850\n",
      "Epoch 220/500\n",
      "96/96 [==============================] - 0s 962us/step - loss: 18.1636 - mse: 18.1636 - val_loss: 17.1968 - val_mse: 17.1968\n",
      "Epoch 221/500\n",
      "96/96 [==============================] - 0s 978us/step - loss: 18.0674 - mse: 18.0674 - val_loss: 17.1090 - val_mse: 17.1090\n",
      "Epoch 222/500\n",
      "96/96 [==============================] - 0s 970us/step - loss: 17.9715 - mse: 17.9715 - val_loss: 17.0209 - val_mse: 17.0209\n",
      "Epoch 223/500\n",
      "96/96 [==============================] - 0s 953us/step - loss: 17.8754 - mse: 17.8754 - val_loss: 16.9332 - val_mse: 16.9332\n",
      "Epoch 224/500\n",
      "96/96 [==============================] - 0s 948us/step - loss: 17.7794 - mse: 17.7794 - val_loss: 16.8457 - val_mse: 16.8457\n",
      "Epoch 225/500\n",
      "96/96 [==============================] - 0s 935us/step - loss: 17.6835 - mse: 17.6835 - val_loss: 16.7582 - val_mse: 16.7582\n",
      "Epoch 226/500\n",
      "96/96 [==============================] - 0s 933us/step - loss: 17.5876 - mse: 17.5876 - val_loss: 16.6716 - val_mse: 16.6716\n",
      "Epoch 227/500\n",
      "96/96 [==============================] - 0s 955us/step - loss: 17.4920 - mse: 17.4920 - val_loss: 16.5846 - val_mse: 16.5846\n",
      "Epoch 228/500\n",
      "96/96 [==============================] - 0s 979us/step - loss: 17.3963 - mse: 17.3963 - val_loss: 16.4983 - val_mse: 16.4983\n",
      "Epoch 229/500\n",
      "96/96 [==============================] - 0s 974us/step - loss: 17.3006 - mse: 17.3006 - val_loss: 16.4113 - val_mse: 16.4113\n",
      "Epoch 230/500\n",
      "96/96 [==============================] - 0s 955us/step - loss: 17.2050 - mse: 17.2050 - val_loss: 16.3247 - val_mse: 16.3247\n",
      "Epoch 231/500\n",
      "96/96 [==============================] - 0s 935us/step - loss: 17.1093 - mse: 17.1093 - val_loss: 16.2387 - val_mse: 16.2387\n",
      "Epoch 232/500\n",
      "96/96 [==============================] - 0s 922us/step - loss: 17.0137 - mse: 17.0137 - val_loss: 16.1523 - val_mse: 16.1523\n",
      "Epoch 233/500\n",
      "96/96 [==============================] - 0s 930us/step - loss: 16.9182 - mse: 16.9182 - val_loss: 16.0666 - val_mse: 16.0666\n",
      "Epoch 234/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 16.8228 - mse: 16.8228 - val_loss: 15.9809 - val_mse: 15.9809\n",
      "Epoch 235/500\n",
      "96/96 [==============================] - 0s 925us/step - loss: 16.7276 - mse: 16.7276 - val_loss: 15.8955 - val_mse: 15.8955\n",
      "Epoch 236/500\n",
      "96/96 [==============================] - 0s 925us/step - loss: 16.6324 - mse: 16.6324 - val_loss: 15.8102 - val_mse: 15.8102\n",
      "Epoch 237/500\n",
      "96/96 [==============================] - 0s 950us/step - loss: 16.5373 - mse: 16.5373 - val_loss: 15.7251 - val_mse: 15.7251\n",
      "Epoch 238/500\n",
      "96/96 [==============================] - 0s 934us/step - loss: 16.4423 - mse: 16.4423 - val_loss: 15.6400 - val_mse: 15.6400\n",
      "Epoch 239/500\n",
      "96/96 [==============================] - 0s 946us/step - loss: 16.3474 - mse: 16.3474 - val_loss: 15.5553 - val_mse: 15.5553\n",
      "Epoch 240/500\n",
      "96/96 [==============================] - 0s 985us/step - loss: 16.2524 - mse: 16.2524 - val_loss: 15.4706 - val_mse: 15.4706\n",
      "Epoch 241/500\n",
      "96/96 [==============================] - 0s 982us/step - loss: 16.1577 - mse: 16.1577 - val_loss: 15.3864 - val_mse: 15.3864\n",
      "Epoch 242/500\n",
      "96/96 [==============================] - 0s 957us/step - loss: 16.0629 - mse: 16.0629 - val_loss: 15.3023 - val_mse: 15.3023\n",
      "Epoch 243/500\n",
      "96/96 [==============================] - 0s 957us/step - loss: 15.9684 - mse: 15.9684 - val_loss: 15.2183 - val_mse: 15.2183\n",
      "Epoch 244/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 15.8739 - mse: 15.8739 - val_loss: 15.1342 - val_mse: 15.1342\n",
      "Epoch 245/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 15.7795 - mse: 15.7795 - val_loss: 15.0511 - val_mse: 15.0511\n",
      "Epoch 246/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 15.6852 - mse: 15.6852 - val_loss: 14.9675 - val_mse: 14.9675\n",
      "Epoch 247/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 15.5910 - mse: 15.5910 - val_loss: 14.8841 - val_mse: 14.8841\n",
      "Epoch 248/500\n",
      "96/96 [==============================] - 0s 994us/step - loss: 15.4969 - mse: 15.4969 - val_loss: 14.8012 - val_mse: 14.8012\n",
      "Epoch 249/500\n",
      "96/96 [==============================] - 0s 996us/step - loss: 15.4029 - mse: 15.4029 - val_loss: 14.7186 - val_mse: 14.7186\n",
      "Epoch 250/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 15.3091 - mse: 15.3091 - val_loss: 14.6360 - val_mse: 14.6360\n",
      "Epoch 251/500\n",
      "96/96 [==============================] - 0s 993us/step - loss: 15.2154 - mse: 15.2154 - val_loss: 14.5535 - val_mse: 14.5535\n",
      "Epoch 252/500\n",
      "96/96 [==============================] - 0s 980us/step - loss: 15.1217 - mse: 15.1217 - val_loss: 14.4716 - val_mse: 14.4716\n",
      "Epoch 253/500\n",
      "96/96 [==============================] - 0s 929us/step - loss: 15.0284 - mse: 15.0284 - val_loss: 14.3890 - val_mse: 14.3890\n",
      "Epoch 254/500\n",
      "96/96 [==============================] - 0s 937us/step - loss: 14.9351 - mse: 14.9351 - val_loss: 14.3071 - val_mse: 14.3071\n",
      "Epoch 255/500\n",
      "96/96 [==============================] - 0s 933us/step - loss: 14.8420 - mse: 14.8420 - val_loss: 14.2257 - val_mse: 14.2257\n",
      "Epoch 256/500\n",
      "96/96 [==============================] - 0s 982us/step - loss: 14.7491 - mse: 14.7491 - val_loss: 14.1443 - val_mse: 14.1443\n",
      "Epoch 257/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 14.6564 - mse: 14.6564 - val_loss: 14.0632 - val_mse: 14.0632\n",
      "Epoch 258/500\n",
      "96/96 [==============================] - 0s 959us/step - loss: 14.5638 - mse: 14.5638 - val_loss: 13.9824 - val_mse: 13.9824\n",
      "Epoch 259/500\n",
      "96/96 [==============================] - 0s 975us/step - loss: 14.4713 - mse: 14.4713 - val_loss: 13.9015 - val_mse: 13.9015\n",
      "Epoch 260/500\n",
      "96/96 [==============================] - 0s 956us/step - loss: 14.3789 - mse: 14.3789 - val_loss: 13.8211 - val_mse: 13.8211\n",
      "Epoch 261/500\n",
      "96/96 [==============================] - 0s 940us/step - loss: 14.2867 - mse: 14.2867 - val_loss: 13.7410 - val_mse: 13.7410\n",
      "Epoch 262/500\n",
      "96/96 [==============================] - 0s 998us/step - loss: 14.1944 - mse: 14.1944 - val_loss: 13.6615 - val_mse: 13.6615\n",
      "Epoch 263/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 14.1025 - mse: 14.1025 - val_loss: 13.5816 - val_mse: 13.5816\n",
      "Epoch 264/500\n",
      "96/96 [==============================] - 0s 976us/step - loss: 14.0108 - mse: 14.0108 - val_loss: 13.5024 - val_mse: 13.5024\n",
      "Epoch 265/500\n",
      "96/96 [==============================] - 0s 988us/step - loss: 13.9193 - mse: 13.9193 - val_loss: 13.4236 - val_mse: 13.4236\n",
      "Epoch 266/500\n",
      "96/96 [==============================] - 0s 952us/step - loss: 13.8281 - mse: 13.8281 - val_loss: 13.3444 - val_mse: 13.3444\n",
      "Epoch 267/500\n",
      "96/96 [==============================] - 0s 963us/step - loss: 13.7370 - mse: 13.7370 - val_loss: 13.2667 - val_mse: 13.2667\n",
      "Epoch 268/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 13.6463 - mse: 13.6463 - val_loss: 13.1885 - val_mse: 13.1885\n",
      "Epoch 269/500\n",
      "96/96 [==============================] - 0s 955us/step - loss: 13.5557 - mse: 13.5557 - val_loss: 13.1106 - val_mse: 13.1106\n",
      "Epoch 270/500\n",
      "96/96 [==============================] - 0s 956us/step - loss: 13.4653 - mse: 13.4653 - val_loss: 13.0328 - val_mse: 13.0328\n",
      "Epoch 271/500\n",
      "96/96 [==============================] - 0s 997us/step - loss: 13.3750 - mse: 13.3750 - val_loss: 12.9559 - val_mse: 12.9559\n",
      "Epoch 272/500\n",
      "96/96 [==============================] - 0s 942us/step - loss: 13.2850 - mse: 13.2850 - val_loss: 12.8791 - val_mse: 12.8791\n",
      "Epoch 273/500\n",
      "96/96 [==============================] - 0s 989us/step - loss: 13.1951 - mse: 13.1951 - val_loss: 12.8027 - val_mse: 12.8027\n",
      "Epoch 274/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 13.1055 - mse: 13.1055 - val_loss: 12.7265 - val_mse: 12.7265\n",
      "Epoch 275/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 13.0161 - mse: 13.0161 - val_loss: 12.6508 - val_mse: 12.6508\n",
      "Epoch 276/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 12.9271 - mse: 12.9271 - val_loss: 12.5748 - val_mse: 12.5748\n",
      "Epoch 277/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 12.8382 - mse: 12.8382 - val_loss: 12.4995 - val_mse: 12.4995\n",
      "Epoch 278/500\n",
      "96/96 [==============================] - 0s 994us/step - loss: 12.7496 - mse: 12.7496 - val_loss: 12.4244 - val_mse: 12.4244\n",
      "Epoch 279/500\n",
      "96/96 [==============================] - 0s 991us/step - loss: 12.6611 - mse: 12.6611 - val_loss: 12.3501 - val_mse: 12.3501\n",
      "Epoch 280/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 12.5730 - mse: 12.5730 - val_loss: 12.2750 - val_mse: 12.2750\n",
      "Epoch 281/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 12.4849 - mse: 12.4849 - val_loss: 12.2007 - val_mse: 12.2007\n",
      "Epoch 282/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 12.3971 - mse: 12.3971 - val_loss: 12.1272 - val_mse: 12.1272\n",
      "Epoch 283/500\n",
      "96/96 [==============================] - 0s 969us/step - loss: 12.3097 - mse: 12.3097 - val_loss: 12.0528 - val_mse: 12.0528\n",
      "Epoch 284/500\n",
      "96/96 [==============================] - 0s 974us/step - loss: 12.2223 - mse: 12.2223 - val_loss: 11.9800 - val_mse: 11.9800\n",
      "Epoch 285/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 12.1355 - mse: 12.1355 - val_loss: 11.9063 - val_mse: 11.9063\n",
      "Epoch 286/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 12.0487 - mse: 12.0487 - val_loss: 11.8336 - val_mse: 11.8336\n",
      "Epoch 287/500\n",
      "96/96 [==============================] - 0s 978us/step - loss: 11.9623 - mse: 11.9623 - val_loss: 11.7617 - val_mse: 11.7617\n",
      "Epoch 288/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 11.8762 - mse: 11.8762 - val_loss: 11.6894 - val_mse: 11.6894\n",
      "Epoch 289/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 11.7903 - mse: 11.7903 - val_loss: 11.6176 - val_mse: 11.6176\n",
      "Epoch 290/500\n",
      "96/96 [==============================] - 0s 981us/step - loss: 11.7047 - mse: 11.7047 - val_loss: 11.5462 - val_mse: 11.5462\n",
      "Epoch 291/500\n",
      "96/96 [==============================] - 0s 991us/step - loss: 11.6195 - mse: 11.6195 - val_loss: 11.4749 - val_mse: 11.4749\n",
      "Epoch 292/500\n",
      "96/96 [==============================] - 0s 988us/step - loss: 11.5345 - mse: 11.5345 - val_loss: 11.4047 - val_mse: 11.4047\n",
      "Epoch 293/500\n",
      "96/96 [==============================] - 0s 972us/step - loss: 11.4498 - mse: 11.4498 - val_loss: 11.3343 - val_mse: 11.3343\n",
      "Epoch 294/500\n",
      "96/96 [==============================] - 0s 999us/step - loss: 11.3654 - mse: 11.3654 - val_loss: 11.2641 - val_mse: 11.2641\n",
      "Epoch 295/500\n",
      "96/96 [==============================] - 0s 976us/step - loss: 11.2812 - mse: 11.2812 - val_loss: 11.1944 - val_mse: 11.1944\n",
      "Epoch 296/500\n",
      "96/96 [==============================] - 0s 970us/step - loss: 11.1973 - mse: 11.1973 - val_loss: 11.1247 - val_mse: 11.1247\n",
      "Epoch 297/500\n",
      "96/96 [==============================] - 0s 984us/step - loss: 11.1138 - mse: 11.1138 - val_loss: 11.0555 - val_mse: 11.0555\n",
      "Epoch 298/500\n",
      "96/96 [==============================] - 0s 953us/step - loss: 11.0306 - mse: 11.0306 - val_loss: 10.9871 - val_mse: 10.9871\n",
      "Epoch 299/500\n",
      "96/96 [==============================] - 0s 927us/step - loss: 10.9478 - mse: 10.9478 - val_loss: 10.9182 - val_mse: 10.9182\n",
      "Epoch 300/500\n",
      "96/96 [==============================] - 0s 923us/step - loss: 10.8654 - mse: 10.8654 - val_loss: 10.8497 - val_mse: 10.8497\n",
      "Epoch 301/500\n",
      "96/96 [==============================] - 0s 953us/step - loss: 10.7832 - mse: 10.7832 - val_loss: 10.7819 - val_mse: 10.7819\n",
      "Epoch 302/500\n",
      "96/96 [==============================] - 0s 919us/step - loss: 10.7012 - mse: 10.7012 - val_loss: 10.7152 - val_mse: 10.7152\n",
      "Epoch 303/500\n",
      "96/96 [==============================] - 0s 942us/step - loss: 10.6197 - mse: 10.6197 - val_loss: 10.6481 - val_mse: 10.6481\n",
      "Epoch 304/500\n",
      "96/96 [==============================] - 0s 963us/step - loss: 10.5384 - mse: 10.5384 - val_loss: 10.5820 - val_mse: 10.5820\n",
      "Epoch 305/500\n",
      "96/96 [==============================] - 0s 961us/step - loss: 10.4576 - mse: 10.4576 - val_loss: 10.5153 - val_mse: 10.5153\n",
      "Epoch 306/500\n",
      "96/96 [==============================] - 0s 956us/step - loss: 10.3769 - mse: 10.3769 - val_loss: 10.4494 - val_mse: 10.4494\n",
      "Epoch 307/500\n",
      "96/96 [==============================] - 0s 975us/step - loss: 10.2968 - mse: 10.2968 - val_loss: 10.3840 - val_mse: 10.3840\n",
      "Epoch 308/500\n",
      "96/96 [==============================] - 0s 972us/step - loss: 10.2169 - mse: 10.2169 - val_loss: 10.3189 - val_mse: 10.3189\n",
      "Epoch 309/500\n",
      "96/96 [==============================] - 0s 956us/step - loss: 10.1374 - mse: 10.1374 - val_loss: 10.2537 - val_mse: 10.2537\n",
      "Epoch 310/500\n",
      "96/96 [==============================] - 0s 973us/step - loss: 10.0580 - mse: 10.0580 - val_loss: 10.1894 - val_mse: 10.1894\n",
      "Epoch 311/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 9.9790 - mse: 9.9790 - val_loss: 10.1252 - val_mse: 10.1252\n",
      "Epoch 312/500\n",
      "96/96 [==============================] - 0s 962us/step - loss: 9.9005 - mse: 9.9005 - val_loss: 10.0609 - val_mse: 10.0609\n",
      "Epoch 313/500\n",
      "96/96 [==============================] - 0s 939us/step - loss: 9.8221 - mse: 9.8221 - val_loss: 9.9977 - val_mse: 9.9977\n",
      "Epoch 314/500\n",
      "96/96 [==============================] - 0s 939us/step - loss: 9.7442 - mse: 9.7442 - val_loss: 9.9339 - val_mse: 9.9339\n",
      "Epoch 315/500\n",
      "96/96 [==============================] - 0s 939us/step - loss: 9.6667 - mse: 9.6667 - val_loss: 9.8709 - val_mse: 9.8709\n",
      "Epoch 316/500\n",
      "96/96 [==============================] - 0s 932us/step - loss: 9.5895 - mse: 9.5895 - val_loss: 9.8088 - val_mse: 9.8088\n",
      "Epoch 317/500\n",
      "96/96 [==============================] - 0s 953us/step - loss: 9.5127 - mse: 9.5127 - val_loss: 9.7464 - val_mse: 9.7464\n",
      "Epoch 318/500\n",
      "96/96 [==============================] - 0s 928us/step - loss: 9.4362 - mse: 9.4362 - val_loss: 9.6848 - val_mse: 9.6848\n",
      "Epoch 319/500\n",
      "96/96 [==============================] - 0s 953us/step - loss: 9.3600 - mse: 9.3600 - val_loss: 9.6228 - val_mse: 9.6228\n",
      "Epoch 320/500\n",
      "96/96 [==============================] - 0s 949us/step - loss: 9.2841 - mse: 9.2841 - val_loss: 9.5613 - val_mse: 9.5613\n",
      "Epoch 321/500\n",
      "96/96 [==============================] - 0s 941us/step - loss: 9.2085 - mse: 9.2085 - val_loss: 9.5005 - val_mse: 9.5005\n",
      "Epoch 322/500\n",
      "96/96 [==============================] - 0s 949us/step - loss: 9.1332 - mse: 9.1332 - val_loss: 9.4400 - val_mse: 9.4400\n",
      "Epoch 323/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 9.0583 - mse: 9.0583 - val_loss: 9.3801 - val_mse: 9.3801\n",
      "Epoch 324/500\n",
      "96/96 [==============================] - 0s 946us/step - loss: 8.9838 - mse: 8.9838 - val_loss: 9.3198 - val_mse: 9.3198\n",
      "Epoch 325/500\n",
      "96/96 [==============================] - 0s 931us/step - loss: 8.9097 - mse: 8.9097 - val_loss: 9.2603 - val_mse: 9.2603\n",
      "Epoch 326/500\n",
      "96/96 [==============================] - 0s 934us/step - loss: 8.8359 - mse: 8.8359 - val_loss: 9.2013 - val_mse: 9.2013\n",
      "Epoch 327/500\n",
      "96/96 [==============================] - 0s 958us/step - loss: 8.7626 - mse: 8.7626 - val_loss: 9.1425 - val_mse: 9.1425\n",
      "Epoch 328/500\n",
      "96/96 [==============================] - 0s 928us/step - loss: 8.6896 - mse: 8.6896 - val_loss: 9.0840 - val_mse: 9.0840\n",
      "Epoch 329/500\n",
      "96/96 [==============================] - 0s 934us/step - loss: 8.6170 - mse: 8.6170 - val_loss: 9.0265 - val_mse: 9.0265\n",
      "Epoch 330/500\n",
      "96/96 [==============================] - 0s 934us/step - loss: 8.5449 - mse: 8.5449 - val_loss: 8.9685 - val_mse: 8.9685\n",
      "Epoch 331/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 8.4731 - mse: 8.4731 - val_loss: 8.9113 - val_mse: 8.9113\n",
      "Epoch 332/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 8.4017 - mse: 8.4017 - val_loss: 8.8546 - val_mse: 8.8546\n",
      "Epoch 333/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 8.3309 - mse: 8.3309 - val_loss: 8.7978 - val_mse: 8.7978\n",
      "Epoch 334/500\n",
      "96/96 [==============================] - 0s 968us/step - loss: 8.2605 - mse: 8.2605 - val_loss: 8.7414 - val_mse: 8.7414\n",
      "Epoch 335/500\n",
      "96/96 [==============================] - 0s 988us/step - loss: 8.1903 - mse: 8.1903 - val_loss: 8.6854 - val_mse: 8.6854\n",
      "Epoch 336/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 8.1206 - mse: 8.1206 - val_loss: 8.6298 - val_mse: 8.6298\n",
      "Epoch 337/500\n",
      "96/96 [==============================] - 0s 960us/step - loss: 8.0514 - mse: 8.0514 - val_loss: 8.5747 - val_mse: 8.5747\n",
      "Epoch 338/500\n",
      "96/96 [==============================] - 0s 984us/step - loss: 7.9825 - mse: 7.9825 - val_loss: 8.5202 - val_mse: 8.5202\n",
      "Epoch 339/500\n",
      "96/96 [==============================] - 0s 927us/step - loss: 7.9142 - mse: 7.9142 - val_loss: 8.4652 - val_mse: 8.4652\n",
      "Epoch 340/500\n",
      "96/96 [==============================] - 0s 928us/step - loss: 7.8462 - mse: 7.8462 - val_loss: 8.4112 - val_mse: 8.4112\n",
      "Epoch 341/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 7.7788 - mse: 7.7788 - val_loss: 8.3578 - val_mse: 8.3578\n",
      "Epoch 342/500\n",
      "96/96 [==============================] - 0s 928us/step - loss: 7.7118 - mse: 7.7118 - val_loss: 8.3044 - val_mse: 8.3044\n",
      "Epoch 343/500\n",
      "96/96 [==============================] - 0s 928us/step - loss: 7.6453 - mse: 7.6453 - val_loss: 8.2515 - val_mse: 8.2515\n",
      "Epoch 344/500\n",
      "96/96 [==============================] - 0s 941us/step - loss: 7.5793 - mse: 7.5793 - val_loss: 8.1988 - val_mse: 8.1988\n",
      "Epoch 345/500\n",
      "96/96 [==============================] - 0s 964us/step - loss: 7.5137 - mse: 7.5137 - val_loss: 8.1471 - val_mse: 8.1471\n",
      "Epoch 346/500\n",
      "96/96 [==============================] - 0s 933us/step - loss: 7.4485 - mse: 7.4485 - val_loss: 8.0954 - val_mse: 8.0954\n",
      "Epoch 347/500\n",
      "96/96 [==============================] - 0s 941us/step - loss: 7.3837 - mse: 7.3837 - val_loss: 8.0446 - val_mse: 8.0446\n",
      "Epoch 348/500\n",
      "96/96 [==============================] - 0s 932us/step - loss: 7.3194 - mse: 7.3194 - val_loss: 7.9933 - val_mse: 7.9933\n",
      "Epoch 349/500\n",
      "96/96 [==============================] - 0s 944us/step - loss: 7.2554 - mse: 7.2554 - val_loss: 7.9428 - val_mse: 7.9428\n",
      "Epoch 350/500\n",
      "96/96 [==============================] - 0s 930us/step - loss: 7.1921 - mse: 7.1921 - val_loss: 7.8925 - val_mse: 7.8925\n",
      "Epoch 351/500\n",
      "96/96 [==============================] - 0s 922us/step - loss: 7.1293 - mse: 7.1293 - val_loss: 7.8429 - val_mse: 7.8429\n",
      "Epoch 352/500\n",
      "96/96 [==============================] - 0s 946us/step - loss: 7.0668 - mse: 7.0668 - val_loss: 7.7932 - val_mse: 7.7932\n",
      "Epoch 353/500\n",
      "96/96 [==============================] - 0s 941us/step - loss: 7.0047 - mse: 7.0047 - val_loss: 7.7441 - val_mse: 7.7441\n",
      "Epoch 354/500\n",
      "96/96 [==============================] - 0s 952us/step - loss: 6.9432 - mse: 6.9432 - val_loss: 7.6952 - val_mse: 7.6952\n",
      "Epoch 355/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 6.8821 - mse: 6.8821 - val_loss: 7.6471 - val_mse: 7.6471\n",
      "Epoch 356/500\n",
      "96/96 [==============================] - 0s 952us/step - loss: 6.8215 - mse: 6.8215 - val_loss: 7.5992 - val_mse: 7.5992\n",
      "Epoch 357/500\n",
      "96/96 [==============================] - 0s 974us/step - loss: 6.7614 - mse: 6.7614 - val_loss: 7.5511 - val_mse: 7.5511\n",
      "Epoch 358/500\n",
      "96/96 [==============================] - 0s 929us/step - loss: 6.7017 - mse: 6.7017 - val_loss: 7.5037 - val_mse: 7.5037\n",
      "Epoch 359/500\n",
      "96/96 [==============================] - 0s 959us/step - loss: 6.6423 - mse: 6.6423 - val_loss: 7.4569 - val_mse: 7.4569\n",
      "Epoch 360/500\n",
      "96/96 [==============================] - 0s 926us/step - loss: 6.5836 - mse: 6.5836 - val_loss: 7.4099 - val_mse: 7.4099\n",
      "Epoch 361/500\n",
      "96/96 [==============================] - 0s 924us/step - loss: 6.5252 - mse: 6.5252 - val_loss: 7.3636 - val_mse: 7.3636\n",
      "Epoch 362/500\n",
      "96/96 [==============================] - 0s 935us/step - loss: 6.4672 - mse: 6.4672 - val_loss: 7.3175 - val_mse: 7.3175\n",
      "Epoch 363/500\n",
      "96/96 [==============================] - 0s 926us/step - loss: 6.4099 - mse: 6.4099 - val_loss: 7.2717 - val_mse: 7.2717\n",
      "Epoch 364/500\n",
      "96/96 [==============================] - 0s 919us/step - loss: 6.3529 - mse: 6.3529 - val_loss: 7.2270 - val_mse: 7.2270\n",
      "Epoch 365/500\n",
      "96/96 [==============================] - 0s 955us/step - loss: 6.2963 - mse: 6.2963 - val_loss: 7.1817 - val_mse: 7.1817\n",
      "Epoch 366/500\n",
      "96/96 [==============================] - 0s 964us/step - loss: 6.2399 - mse: 6.2399 - val_loss: 7.1370 - val_mse: 7.1370\n",
      "Epoch 367/500\n",
      "96/96 [==============================] - 0s 973us/step - loss: 6.1840 - mse: 6.1840 - val_loss: 7.0928 - val_mse: 7.0928\n",
      "Epoch 368/500\n",
      "96/96 [==============================] - 0s 952us/step - loss: 6.1286 - mse: 6.1286 - val_loss: 7.0487 - val_mse: 7.0487\n",
      "Epoch 369/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 6.0737 - mse: 6.0737 - val_loss: 7.0047 - val_mse: 7.0047\n",
      "Epoch 370/500\n",
      "96/96 [==============================] - 0s 967us/step - loss: 6.0193 - mse: 6.0193 - val_loss: 6.9614 - val_mse: 6.9614\n",
      "Epoch 371/500\n",
      "96/96 [==============================] - 0s 941us/step - loss: 5.9653 - mse: 5.9653 - val_loss: 6.9180 - val_mse: 6.9180\n",
      "Epoch 372/500\n",
      "96/96 [==============================] - 0s 963us/step - loss: 5.9118 - mse: 5.9118 - val_loss: 6.8753 - val_mse: 6.8753\n",
      "Epoch 373/500\n",
      "96/96 [==============================] - 0s 959us/step - loss: 5.8588 - mse: 5.8588 - val_loss: 6.8330 - val_mse: 6.8330\n",
      "Epoch 374/500\n",
      "96/96 [==============================] - 0s 954us/step - loss: 5.8063 - mse: 5.8063 - val_loss: 6.7907 - val_mse: 6.7907\n",
      "Epoch 375/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 5.7541 - mse: 5.7541 - val_loss: 6.7488 - val_mse: 6.7488\n",
      "Epoch 376/500\n",
      "96/96 [==============================] - 0s 935us/step - loss: 5.7024 - mse: 5.7024 - val_loss: 6.7078 - val_mse: 6.7078\n",
      "Epoch 377/500\n",
      "96/96 [==============================] - 0s 924us/step - loss: 5.6513 - mse: 5.6513 - val_loss: 6.6666 - val_mse: 6.6666\n",
      "Epoch 378/500\n",
      "96/96 [==============================] - 0s 975us/step - loss: 5.6006 - mse: 5.6006 - val_loss: 6.6259 - val_mse: 6.6259\n",
      "Epoch 379/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 5.5503 - mse: 5.5503 - val_loss: 6.5855 - val_mse: 6.5855\n",
      "Epoch 380/500\n",
      "96/96 [==============================] - 0s 991us/step - loss: 5.5006 - mse: 5.5006 - val_loss: 6.5451 - val_mse: 6.5451\n",
      "Epoch 381/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 5.4512 - mse: 5.4512 - val_loss: 6.5057 - val_mse: 6.5057\n",
      "Epoch 382/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 5.4026 - mse: 5.4026 - val_loss: 6.4660 - val_mse: 6.4660\n",
      "Epoch 383/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 5.3543 - mse: 5.3543 - val_loss: 6.4270 - val_mse: 6.4270\n",
      "Epoch 384/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 5.3066 - mse: 5.3066 - val_loss: 6.3886 - val_mse: 6.3886\n",
      "Epoch 385/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 5.2596 - mse: 5.2596 - val_loss: 6.3503 - val_mse: 6.3503\n",
      "Epoch 386/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 5.2131 - mse: 5.2131 - val_loss: 6.3123 - val_mse: 6.3123\n",
      "Epoch 387/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 5.1670 - mse: 5.1670 - val_loss: 6.2749 - val_mse: 6.2749\n",
      "Epoch 388/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 5.1214 - mse: 5.1214 - val_loss: 6.2371 - val_mse: 6.2371\n",
      "Epoch 389/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 5.0762 - mse: 5.0762 - val_loss: 6.2003 - val_mse: 6.2003\n",
      "Epoch 390/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 5.0316 - mse: 5.0316 - val_loss: 6.1632 - val_mse: 6.1632\n",
      "Epoch 391/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 4.9874 - mse: 4.9874 - val_loss: 6.1272 - val_mse: 6.1272\n",
      "Epoch 392/500\n",
      "96/96 [==============================] - 0s 985us/step - loss: 4.9438 - mse: 4.9438 - val_loss: 6.0908 - val_mse: 6.0908\n",
      "Epoch 393/500\n",
      "96/96 [==============================] - 0s 990us/step - loss: 4.9006 - mse: 4.9006 - val_loss: 6.0550 - val_mse: 6.0550\n",
      "Epoch 394/500\n",
      "96/96 [==============================] - 0s 935us/step - loss: 4.8580 - mse: 4.8580 - val_loss: 6.0195 - val_mse: 6.0195\n",
      "Epoch 395/500\n",
      "96/96 [==============================] - 0s 963us/step - loss: 4.8160 - mse: 4.8160 - val_loss: 5.9844 - val_mse: 5.9844\n",
      "Epoch 396/500\n",
      "96/96 [==============================] - 0s 949us/step - loss: 4.7745 - mse: 4.7745 - val_loss: 5.9492 - val_mse: 5.9492\n",
      "Epoch 397/500\n",
      "96/96 [==============================] - 0s 943us/step - loss: 4.7336 - mse: 4.7336 - val_loss: 5.9147 - val_mse: 5.9147\n",
      "Epoch 398/500\n",
      "96/96 [==============================] - 0s 939us/step - loss: 4.6931 - mse: 4.6931 - val_loss: 5.8807 - val_mse: 5.8807\n",
      "Epoch 399/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 4.6531 - mse: 4.6531 - val_loss: 5.8469 - val_mse: 5.8469\n",
      "Epoch 400/500\n",
      "96/96 [==============================] - 0s 962us/step - loss: 4.6136 - mse: 4.6136 - val_loss: 5.8136 - val_mse: 5.8136\n",
      "Epoch 401/500\n",
      "96/96 [==============================] - 0s 966us/step - loss: 4.5746 - mse: 4.5746 - val_loss: 5.7801 - val_mse: 5.7801\n",
      "Epoch 402/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 4.5361 - mse: 4.5361 - val_loss: 5.7476 - val_mse: 5.7476\n",
      "Epoch 403/500\n",
      "96/96 [==============================] - 0s 953us/step - loss: 4.4981 - mse: 4.4981 - val_loss: 5.7148 - val_mse: 5.7148\n",
      "Epoch 404/500\n",
      "96/96 [==============================] - 0s 943us/step - loss: 4.4605 - mse: 4.4605 - val_loss: 5.6827 - val_mse: 5.6827\n",
      "Epoch 405/500\n",
      "96/96 [==============================] - 0s 943us/step - loss: 4.4235 - mse: 4.4235 - val_loss: 5.6511 - val_mse: 5.6511\n",
      "Epoch 406/500\n",
      "96/96 [==============================] - 0s 952us/step - loss: 4.3870 - mse: 4.3870 - val_loss: 5.6193 - val_mse: 5.6193\n",
      "Epoch 407/500\n",
      "96/96 [==============================] - 0s 935us/step - loss: 4.3509 - mse: 4.3509 - val_loss: 5.5881 - val_mse: 5.5881\n",
      "Epoch 408/500\n",
      "96/96 [==============================] - 0s 924us/step - loss: 4.3154 - mse: 4.3154 - val_loss: 5.5577 - val_mse: 5.5577\n",
      "Epoch 409/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 4.2804 - mse: 4.2804 - val_loss: 5.5268 - val_mse: 5.5268\n",
      "Epoch 410/500\n",
      "96/96 [==============================] - 0s 932us/step - loss: 4.2460 - mse: 4.2460 - val_loss: 5.4961 - val_mse: 5.4961\n",
      "Epoch 411/500\n",
      "96/96 [==============================] - 0s 940us/step - loss: 4.2120 - mse: 4.2120 - val_loss: 5.4668 - val_mse: 5.4668\n",
      "Epoch 412/500\n",
      "96/96 [==============================] - 0s 926us/step - loss: 4.1786 - mse: 4.1786 - val_loss: 5.4370 - val_mse: 5.4370\n",
      "Epoch 413/500\n",
      "96/96 [==============================] - 0s 945us/step - loss: 4.1457 - mse: 4.1457 - val_loss: 5.4076 - val_mse: 5.4076\n",
      "Epoch 414/500\n",
      "96/96 [==============================] - 0s 940us/step - loss: 4.1133 - mse: 4.1133 - val_loss: 5.3790 - val_mse: 5.3790\n",
      "Epoch 415/500\n",
      "96/96 [==============================] - 0s 939us/step - loss: 4.0814 - mse: 4.0814 - val_loss: 5.3499 - val_mse: 5.3499\n",
      "Epoch 416/500\n",
      "96/96 [==============================] - 0s 959us/step - loss: 4.0499 - mse: 4.0499 - val_loss: 5.3222 - val_mse: 5.3222\n",
      "Epoch 417/500\n",
      "96/96 [==============================] - 0s 945us/step - loss: 4.0190 - mse: 4.0190 - val_loss: 5.2942 - val_mse: 5.2942\n",
      "Epoch 418/500\n",
      "96/96 [==============================] - 0s 941us/step - loss: 3.9885 - mse: 3.9885 - val_loss: 5.2663 - val_mse: 5.2663\n",
      "Epoch 419/500\n",
      "96/96 [==============================] - 0s 920us/step - loss: 3.9585 - mse: 3.9585 - val_loss: 5.2390 - val_mse: 5.2390\n",
      "Epoch 420/500\n",
      "96/96 [==============================] - 0s 940us/step - loss: 3.9289 - mse: 3.9289 - val_loss: 5.2119 - val_mse: 5.2119\n",
      "Epoch 421/500\n",
      "96/96 [==============================] - 0s 930us/step - loss: 3.8998 - mse: 3.8998 - val_loss: 5.1854 - val_mse: 5.1854\n",
      "Epoch 422/500\n",
      "96/96 [==============================] - 0s 986us/step - loss: 3.8712 - mse: 3.8712 - val_loss: 5.1587 - val_mse: 5.1587\n",
      "Epoch 423/500\n",
      "96/96 [==============================] - 0s 948us/step - loss: 3.8430 - mse: 3.8430 - val_loss: 5.1325 - val_mse: 5.1325\n",
      "Epoch 424/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 3.8153 - mse: 3.8153 - val_loss: 5.1067 - val_mse: 5.1067\n",
      "Epoch 425/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 3.7879 - mse: 3.7879 - val_loss: 5.0812 - val_mse: 5.0812\n",
      "Epoch 426/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 3.7612 - mse: 3.7612 - val_loss: 5.0561 - val_mse: 5.0561\n",
      "Epoch 427/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 3.7348 - mse: 3.7348 - val_loss: 5.0308 - val_mse: 5.0308\n",
      "Epoch 428/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 3.7089 - mse: 3.7089 - val_loss: 5.0059 - val_mse: 5.0059\n",
      "Epoch 429/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 3.6833 - mse: 3.6833 - val_loss: 4.9815 - val_mse: 4.9815\n",
      "Epoch 430/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 3.6582 - mse: 3.6582 - val_loss: 4.9577 - val_mse: 4.9577\n",
      "Epoch 431/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 3.6336 - mse: 3.6336 - val_loss: 4.9336 - val_mse: 4.9336\n",
      "Epoch 432/500\n",
      "96/96 [==============================] - 0s 991us/step - loss: 3.6094 - mse: 3.6094 - val_loss: 4.9101 - val_mse: 4.9101\n",
      "Epoch 433/500\n",
      "96/96 [==============================] - 0s 966us/step - loss: 3.5856 - mse: 3.5856 - val_loss: 4.8869 - val_mse: 4.8869\n",
      "Epoch 434/500\n",
      "96/96 [==============================] - 0s 924us/step - loss: 3.5623 - mse: 3.5623 - val_loss: 4.8637 - val_mse: 4.8637\n",
      "Epoch 435/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 3.5393 - mse: 3.5393 - val_loss: 4.8410 - val_mse: 4.8410\n",
      "Epoch 436/500\n",
      "96/96 [==============================] - 0s 929us/step - loss: 3.5168 - mse: 3.5168 - val_loss: 4.8185 - val_mse: 4.8185\n",
      "Epoch 437/500\n",
      "96/96 [==============================] - 0s 956us/step - loss: 3.4946 - mse: 3.4946 - val_loss: 4.7971 - val_mse: 4.7971\n",
      "Epoch 438/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 3.4729 - mse: 3.4729 - val_loss: 4.7746 - val_mse: 4.7746\n",
      "Epoch 439/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 3.4515 - mse: 3.4515 - val_loss: 4.7536 - val_mse: 4.7536\n",
      "Epoch 440/500\n",
      "96/96 [==============================] - 0s 928us/step - loss: 3.4305 - mse: 3.4305 - val_loss: 4.7321 - val_mse: 4.7321\n",
      "Epoch 441/500\n",
      "96/96 [==============================] - 0s 943us/step - loss: 3.4099 - mse: 3.4099 - val_loss: 4.7112 - val_mse: 4.7112\n",
      "Epoch 442/500\n",
      "96/96 [==============================] - 0s 944us/step - loss: 3.3897 - mse: 3.3897 - val_loss: 4.6903 - val_mse: 4.6903\n",
      "Epoch 443/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 3.3699 - mse: 3.3699 - val_loss: 4.6696 - val_mse: 4.6696\n",
      "Epoch 444/500\n",
      "96/96 [==============================] - 0s 977us/step - loss: 3.3503 - mse: 3.3503 - val_loss: 4.6493 - val_mse: 4.6493\n",
      "Epoch 445/500\n",
      "96/96 [==============================] - 0s 934us/step - loss: 3.3312 - mse: 3.3312 - val_loss: 4.6296 - val_mse: 4.6296\n",
      "Epoch 446/500\n",
      "96/96 [==============================] - 0s 920us/step - loss: 3.3125 - mse: 3.3125 - val_loss: 4.6100 - val_mse: 4.6100\n",
      "Epoch 447/500\n",
      "96/96 [==============================] - 0s 911us/step - loss: 3.2941 - mse: 3.2941 - val_loss: 4.5906 - val_mse: 4.5906\n",
      "Epoch 448/500\n",
      "96/96 [==============================] - 0s 929us/step - loss: 3.2760 - mse: 3.2760 - val_loss: 4.5713 - val_mse: 4.5713\n",
      "Epoch 449/500\n",
      "96/96 [==============================] - 0s 954us/step - loss: 3.2582 - mse: 3.2582 - val_loss: 4.5525 - val_mse: 4.5525\n",
      "Epoch 450/500\n",
      "96/96 [==============================] - 0s 927us/step - loss: 3.2408 - mse: 3.2408 - val_loss: 4.5338 - val_mse: 4.5338\n",
      "Epoch 451/500\n",
      "96/96 [==============================] - 0s 937us/step - loss: 3.2236 - mse: 3.2236 - val_loss: 4.5153 - val_mse: 4.5153\n",
      "Epoch 452/500\n",
      "96/96 [==============================] - 0s 920us/step - loss: 3.2069 - mse: 3.2069 - val_loss: 4.4970 - val_mse: 4.4970\n",
      "Epoch 453/500\n",
      "96/96 [==============================] - 0s 922us/step - loss: 3.1903 - mse: 3.1903 - val_loss: 4.4788 - val_mse: 4.4788\n",
      "Epoch 454/500\n",
      "96/96 [==============================] - 0s 926us/step - loss: 3.1742 - mse: 3.1742 - val_loss: 4.4609 - val_mse: 4.4609\n",
      "Epoch 455/500\n",
      "96/96 [==============================] - 0s 916us/step - loss: 3.1584 - mse: 3.1584 - val_loss: 4.4430 - val_mse: 4.4430\n",
      "Epoch 456/500\n",
      "96/96 [==============================] - 0s 925us/step - loss: 3.1428 - mse: 3.1428 - val_loss: 4.4259 - val_mse: 4.4259\n",
      "Epoch 457/500\n",
      "96/96 [==============================] - 0s 922us/step - loss: 3.1276 - mse: 3.1276 - val_loss: 4.4085 - val_mse: 4.4085\n",
      "Epoch 458/500\n",
      "96/96 [==============================] - 0s 919us/step - loss: 3.1125 - mse: 3.1125 - val_loss: 4.3912 - val_mse: 4.3912\n",
      "Epoch 459/500\n",
      "96/96 [==============================] - 0s 920us/step - loss: 3.0978 - mse: 3.0978 - val_loss: 4.3743 - val_mse: 4.3743\n",
      "Epoch 460/500\n",
      "96/96 [==============================] - 0s 918us/step - loss: 3.0833 - mse: 3.0833 - val_loss: 4.3579 - val_mse: 4.3579\n",
      "Epoch 461/500\n",
      "96/96 [==============================] - 0s 947us/step - loss: 3.0691 - mse: 3.0691 - val_loss: 4.3411 - val_mse: 4.3411\n",
      "Epoch 462/500\n",
      "96/96 [==============================] - 0s 927us/step - loss: 3.0551 - mse: 3.0551 - val_loss: 4.3251 - val_mse: 4.3251\n",
      "Epoch 463/500\n",
      "96/96 [==============================] - 0s 949us/step - loss: 3.0414 - mse: 3.0414 - val_loss: 4.3090 - val_mse: 4.3090\n",
      "Epoch 464/500\n",
      "96/96 [==============================] - 0s 929us/step - loss: 3.0280 - mse: 3.0280 - val_loss: 4.2929 - val_mse: 4.2929\n",
      "Epoch 465/500\n",
      "96/96 [==============================] - 0s 926us/step - loss: 3.0147 - mse: 3.0147 - val_loss: 4.2775 - val_mse: 4.2775\n",
      "Epoch 466/500\n",
      "96/96 [==============================] - 0s 975us/step - loss: 3.0016 - mse: 3.0016 - val_loss: 4.2615 - val_mse: 4.2615\n",
      "Epoch 467/500\n",
      "96/96 [==============================] - 0s 931us/step - loss: 2.9889 - mse: 2.9889 - val_loss: 4.2457 - val_mse: 4.2457\n",
      "Epoch 468/500\n",
      "96/96 [==============================] - 0s 905us/step - loss: 2.9762 - mse: 2.9762 - val_loss: 4.2312 - val_mse: 4.2312\n",
      "Epoch 469/500\n",
      "96/96 [==============================] - 0s 930us/step - loss: 2.9638 - mse: 2.9638 - val_loss: 4.2157 - val_mse: 4.2157\n",
      "Epoch 470/500\n",
      "96/96 [==============================] - 0s 916us/step - loss: 2.9517 - mse: 2.9517 - val_loss: 4.2006 - val_mse: 4.2006\n",
      "Epoch 471/500\n",
      "96/96 [==============================] - 0s 923us/step - loss: 2.9397 - mse: 2.9397 - val_loss: 4.1862 - val_mse: 4.1862\n",
      "Epoch 472/500\n",
      "96/96 [==============================] - 0s 927us/step - loss: 2.9279 - mse: 2.9279 - val_loss: 4.1716 - val_mse: 4.1716\n",
      "Epoch 473/500\n",
      "96/96 [==============================] - 0s 926us/step - loss: 2.9163 - mse: 2.9163 - val_loss: 4.1572 - val_mse: 4.1572\n",
      "Epoch 474/500\n",
      "96/96 [==============================] - 0s 934us/step - loss: 2.9048 - mse: 2.9048 - val_loss: 4.1429 - val_mse: 4.1429\n",
      "Epoch 475/500\n",
      "96/96 [==============================] - 0s 933us/step - loss: 2.8935 - mse: 2.8935 - val_loss: 4.1284 - val_mse: 4.1284\n",
      "Epoch 476/500\n",
      "96/96 [==============================] - 0s 914us/step - loss: 2.8823 - mse: 2.8823 - val_loss: 4.1141 - val_mse: 4.1141\n",
      "Epoch 477/500\n",
      "96/96 [==============================] - 0s 919us/step - loss: 2.8712 - mse: 2.8712 - val_loss: 4.1000 - val_mse: 4.1000\n",
      "Epoch 478/500\n",
      "96/96 [==============================] - 0s 911us/step - loss: 2.8604 - mse: 2.8604 - val_loss: 4.0862 - val_mse: 4.0862\n",
      "Epoch 479/500\n",
      "96/96 [==============================] - 0s 922us/step - loss: 2.8497 - mse: 2.8497 - val_loss: 4.0722 - val_mse: 4.0722\n",
      "Epoch 480/500\n",
      "96/96 [==============================] - 0s 927us/step - loss: 2.8391 - mse: 2.8391 - val_loss: 4.0590 - val_mse: 4.0590\n",
      "Epoch 481/500\n",
      "96/96 [==============================] - 0s 924us/step - loss: 2.8287 - mse: 2.8287 - val_loss: 4.0453 - val_mse: 4.0453\n",
      "Epoch 482/500\n",
      "96/96 [==============================] - 0s 922us/step - loss: 2.8183 - mse: 2.8183 - val_loss: 4.0318 - val_mse: 4.0318\n",
      "Epoch 483/500\n",
      "96/96 [==============================] - 0s 922us/step - loss: 2.8081 - mse: 2.8081 - val_loss: 4.0181 - val_mse: 4.0181\n",
      "Epoch 484/500\n",
      "96/96 [==============================] - 0s 918us/step - loss: 2.7980 - mse: 2.7980 - val_loss: 4.0052 - val_mse: 4.0052\n",
      "Epoch 485/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 2.7881 - mse: 2.7881 - val_loss: 3.9920 - val_mse: 3.9920\n",
      "Epoch 486/500\n",
      "96/96 [==============================] - 0s 944us/step - loss: 2.7782 - mse: 2.7782 - val_loss: 3.9787 - val_mse: 3.9787\n",
      "Epoch 487/500\n",
      "96/96 [==============================] - 0s 934us/step - loss: 2.7684 - mse: 2.7684 - val_loss: 3.9661 - val_mse: 3.9661\n",
      "Epoch 488/500\n",
      "96/96 [==============================] - 0s 952us/step - loss: 2.7589 - mse: 2.7589 - val_loss: 3.9528 - val_mse: 3.9528\n",
      "Epoch 489/500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 2.7492 - mse: 2.7492 - val_loss: 3.9404 - val_mse: 3.9404\n",
      "Epoch 490/500\n",
      "96/96 [==============================] - 0s 930us/step - loss: 2.7398 - mse: 2.7398 - val_loss: 3.9274 - val_mse: 3.9274\n",
      "Epoch 491/500\n",
      "96/96 [==============================] - 0s 942us/step - loss: 2.7304 - mse: 2.7304 - val_loss: 3.9147 - val_mse: 3.9147\n",
      "Epoch 492/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 2.7212 - mse: 2.7212 - val_loss: 3.9025 - val_mse: 3.9025\n",
      "Epoch 493/500\n",
      "96/96 [==============================] - 0s 940us/step - loss: 2.7120 - mse: 2.7120 - val_loss: 3.8901 - val_mse: 3.8901\n",
      "Epoch 494/500\n",
      "96/96 [==============================] - 0s 938us/step - loss: 2.7029 - mse: 2.7029 - val_loss: 3.8775 - val_mse: 3.8775\n",
      "Epoch 495/500\n",
      "96/96 [==============================] - 0s 947us/step - loss: 2.6938 - mse: 2.6938 - val_loss: 3.8653 - val_mse: 3.8653\n",
      "Epoch 496/500\n",
      "96/96 [==============================] - 0s 942us/step - loss: 2.6849 - mse: 2.6849 - val_loss: 3.8531 - val_mse: 3.8531\n",
      "Epoch 497/500\n",
      "96/96 [==============================] - 0s 935us/step - loss: 2.6760 - mse: 2.6760 - val_loss: 3.8411 - val_mse: 3.8411\n",
      "Epoch 498/500\n",
      "96/96 [==============================] - 0s 941us/step - loss: 2.6672 - mse: 2.6672 - val_loss: 3.8292 - val_mse: 3.8292\n",
      "Epoch 499/500\n",
      "96/96 [==============================] - 0s 951us/step - loss: 2.6585 - mse: 2.6585 - val_loss: 3.8170 - val_mse: 3.8170\n",
      "Epoch 500/500\n",
      "96/96 [==============================] - 0s 933us/step - loss: 2.6499 - mse: 2.6499 - val_loss: 3.8054 - val_mse: 3.8054\n"
     ]
    }
   ],
   "source": [
    "# Fit the model using 50 epochs and the training data\n",
    "model = nn.fit(X_train_scaled, y_resampled, epochs=500, validation_data=(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate the model using the test data to determine the model’s loss and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:12:13.166539400Z",
     "start_time": "2023-11-26T10:12:13.072006200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 - 0s - loss: 3.8054 - mse: 3.8054 - 21ms/epoch - 2ms/step\n",
      "Loss: 3.8054003715515137, Accuracy: 3.8054003715515137\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_mse = nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Save and export your model to an HDF5 file, and name the file `student_loans.h5`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:12:13.168043900Z",
     "start_time": "2023-11-26T10:12:13.166539400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the model's file path\n",
    "file_path = Path(\"saved_models/student_loans.h5\")\n",
    "\n",
    "# Export your model to a HDF5 file\n",
    "nn.save(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Predict Loan Repayment Success by Using your Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Reload your saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:12:13.217067100Z",
     "start_time": "2023-11-26T10:12:13.166539400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the model's file path\n",
    "file_path = Path(\"saved_models/student_loans.h5\")\n",
    "\n",
    "# Load the model to a new object\n",
    "nn_imported = tf.keras.models.load_model(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Make predictions on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:12:13.244581500Z",
     "start_time": "2023-11-26T10:12:13.217067100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 653us/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testing data\n",
    "predictions = nn_imported.predict(X_test_scaled).round().astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a DataFrame to compare the predictions with the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:12:13.308064600Z",
     "start_time": "2023-11-26T10:12:13.248581500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictions</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      predictions  actual\n",
       "551             3       6\n",
       "1413            6       5\n",
       "1090            7       8\n",
       "1369            5       4\n",
       "536             4       5\n",
       "...           ...     ...\n",
       "624             5       5\n",
       "1532            3       6\n",
       "1073            2       6\n",
       "839             4       5\n",
       "40              4       5\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to compare the predictions with the actual values\n",
    "results = pd.DataFrame({\"predictions\": predictions.ravel(), \"actual\": y_test})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Display a sample of the DataFrame you created in step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:12:13.308064600Z",
     "start_time": "2023-11-26T10:12:13.307560200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictions</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      predictions  actual\n",
       "551             3       6\n",
       "1413            6       5\n",
       "1090            7       8\n",
       "1369            5       4\n",
       "536             4       5\n",
       "...           ...     ...\n",
       "996             7       7\n",
       "148             3       6\n",
       "618             5       5\n",
       "135             3       5\n",
       "1060            8       6\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display sample data\n",
    "results.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T10:12:13.309069200Z",
     "start_time": "2023-11-26T10:12:13.307560200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
